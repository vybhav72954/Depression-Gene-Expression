{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "606b7348-db3e-44bd-b97d-3517e9ad4742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import (\n",
    "    StratifiedKFold, LeaveOneOut, cross_val_score, \n",
    "    GridSearchCV, cross_validate\n",
    ")\n",
    "from sklearn.feature_selection import (\n",
    "    SelectKBest, f_classif, mutual_info_classif,\n",
    "    RFE, SelectFromModel, VarianceThreshold\n",
    ")\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression, ElasticNet\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, roc_auc_score, matthews_corrcoef,\n",
    "    confusion_matrix, classification_report, make_scorer\n",
    ")\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.utils import resample\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "import warnings\n",
    "import pickle\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f7f797-c7d6-4a0b-8aa8-49e55bcbf24c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d1504928-4d01-43d3-86ac-f60d191e8d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#matrix_data = matrix_data.drop(index=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "e26ace0e-5da7-4d10-821c-9eae31e49861",
   "metadata": {},
   "outputs": [],
   "source": [
    "#matrix_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "8d9f003d-0a1a-44b0-a0a2-9d477e59247f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bgx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "46a42b6d-9ccb-4d3b-9a57-57d86672026a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#matrix_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "a003e3ab-46a9-4ba3-a3fb-e8b99954f79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#matrix_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "c9118deb-fab6-429b-b432-43543f8656b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bgx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "90987454-cbb7-4bc9-9678-fef1a52c2a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bgx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "c8ba9f17-628f-41dc-96c3-59f300b0d2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bgx.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "da9ee656-26b8-48dd-a3db-c3972ac902ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(matrix_data.index.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "d2e12fc4-fcef-45de-82e6-182893dfe522",
   "metadata": {},
   "outputs": [],
   "source": [
    "#matrix_data.index.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "8676096e-5074-45ee-ad48-2795ce7c1023",
   "metadata": {},
   "outputs": [],
   "source": [
    "#matrix_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "da189263-ffa8-4b2d-9c01-50e5d11958fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "29bf452c-a3ef-4cbc-be6b-bf3ebde906a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_data[['Symbol', 'var']].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "fdd0b7b1-6ff5-4fc3-84b0-cf3a4b3b23c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse_cols.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9844ff75-b271-4afd-9365-892be4d6c4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42, 25159)\n",
      "Index(['GSM1318948', 'GSM1318949', 'GSM1318950', 'GSM1318951', 'GSM1318952'], dtype='object')\n",
      "Index(['OR6C68', 'SMC1B', 'LOC652184', 'SMR3A', 'LCE1D'], dtype='object', name='Symbol')\n"
     ]
    }
   ],
   "source": [
    "# print(normal_data.shape)\n",
    "# print(normal_data.index[:5])\n",
    "# print(normal_data.columns[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4882da22-df81-4872-9aa5-090030f3bc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normal_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "dc8348a1-4820-433c-9304-e8246a578ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normal_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "1308839a-ad8e-4cfc-b2b1-86d8be8a4a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(normal_data.shape)\n",
    "#normal_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "c71474ee-667b-4438-95cc-6c0a53c82627",
   "metadata": {},
   "outputs": [],
   "source": [
    "#matrix_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706a48ab-4ea0-4ad0-9a64-a1431a9740c5",
   "metadata": {},
   "source": [
    "with open(\"data\\\\GSE54564_series_matrix.txt\", 'r') as f:\n",
    "    for line in f:\n",
    "        if \"characteristics_ch1\" in line:\n",
    "            print(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "f32592ac-d129-4cae-a64e-089b72139b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(labels)\n",
    "#print(len(labels))\n",
    "#labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "29073623-1dc0-4dc7-aee7-cac7d3e6fc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number = pd.Series(labels).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "e80e5029-e2a1-4f94-899f-46be34753495",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c6c32c55-3b2c-4bc6-80af-f1dfc57b5050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "279e6200-c43e-4190-af18-e44442efc5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(ttest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2977124a-dddd-4b04-8505-3d86a402f0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gse54564_data(data1_path, data2_path, data3_path):\n",
    "    \"\"\"Load GSE54564 data\"\"\"\n",
    "    \n",
    "    print(\"Loading GSE54564 data...\")\n",
    "    \n",
    "    matrix_data = pd.read_csv(data2_path, sep=\"\\t\", comment=\"!\", index_col=0)\n",
    "    print(f\"Expression matrix: {matrix_data.shape}\")\n",
    "    \n",
    "    labels = []\n",
    "    with open(data2_path, 'r') as f:\n",
    "        for line in f:\n",
    "            if \"disease state\" in line.lower():\n",
    "                labels = line.strip().split(\"\\t\")[1:]\n",
    "                break\n",
    "    \n",
    "    y = (pd.Series(labels, index=matrix_data.columns)\n",
    "         .str.replace('\"', '')\n",
    "         .str.replace('disease state:', \"\")\n",
    "         .str.strip()\n",
    "         .map({\"MDD case\": 1, \"Control\": 0}))\n",
    "    \n",
    "    print(f\"Labels: {y.value_counts().to_dict()}\")\n",
    "    \n",
    "    with open(data3_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    start = None\n",
    "    for i, line in enumerate(lines):\n",
    "        if line.strip() == \"[Probes]\":\n",
    "            start = i + 1\n",
    "            break\n",
    "    \n",
    "    bgx = pd.read_csv(data3_path, sep=\"\\t\", skiprows=start, engine=\"python\")\n",
    "    print(f\"BGX annotation: {bgx.shape}\")\n",
    "    \n",
    "    probe_to_gene = {}\n",
    "    if 'Probe_Id' in bgx.columns and 'Symbol' in bgx.columns:\n",
    "        probe_to_gene = dict(zip(bgx['Probe_Id'], bgx['Symbol']))\n",
    "    elif 'Name' in bgx.columns and 'Symbol' in bgx.columns:\n",
    "        probe_to_gene = dict(zip(bgx['Name'], bgx['Symbol']))\n",
    "    \n",
    "    gene_names = []\n",
    "    valid_probes = []\n",
    "    for probe in matrix_data.index:\n",
    "        if probe in probe_to_gene and pd.notna(probe_to_gene[probe]):\n",
    "            gene_names.append(probe_to_gene[probe])\n",
    "            valid_probes.append(probe)\n",
    "    \n",
    "    X = matrix_data.loc[valid_probes].T.values\n",
    "    \n",
    "    print(f\"Final shape: X={X.shape}, y={y.shape}, genes={len(gene_names)}\")\n",
    "    \n",
    "    return X, y.values, gene_names, probe_to_gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c904f9d8-c8c1-4395-ab64-7da6d06f7a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_expression_data(X, gene_names=None, remove_low_var=True, var_threshold=0.1):\n",
    "    \"\"\"Preprocess expression data with log2 transform, z-score normalization, and variance filtering\"\"\"\n",
    "    \n",
    "    print(\"\\nPreprocessing expression data...\")\n",
    "    print(f\"Input: {X.shape}, gene_names: {len(gene_names) if gene_names else None}\")\n",
    "    \n",
    "    if np.max(X) > 100:\n",
    "        X = np.log2(X + 1)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_normalized = scaler.fit_transform(X)\n",
    "    \n",
    "    if remove_low_var:\n",
    "        gene_variances = np.var(X_normalized, axis=0)\n",
    "        high_var_mask = gene_variances > var_threshold\n",
    "        \n",
    "        n_before = X_normalized.shape[1]\n",
    "        n_after = np.sum(high_var_mask)\n",
    "        \n",
    "        print(f\"Genes: {n_before} -> {n_after} (removed {n_before - n_after})\")\n",
    "        \n",
    "        X_filtered = X_normalized[:, high_var_mask]\n",
    "        \n",
    "        if gene_names is not None:\n",
    "            if len(gene_names) != n_before:\n",
    "                print(f\"WARNING: gene_names length ({len(gene_names)}) != features ({n_before})\")\n",
    "                gene_names = gene_names[:n_before]\n",
    "            \n",
    "            gene_names_filtered = [gene_names[i] for i in range(len(gene_names)) if high_var_mask[i]]\n",
    "            \n",
    "            assert len(gene_names_filtered) == X_filtered.shape[1], \\\n",
    "                f\"Mismatch: {len(gene_names_filtered)} names vs {X_filtered.shape[1]} features\"\n",
    "            \n",
    "            return X_filtered, gene_names_filtered\n",
    "        \n",
    "        return X_filtered, None\n",
    "    \n",
    "    return X_normalized, gene_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "233d6571-f5d4-40fc-bb7a-9030bad1d13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP 1: LOADING DATA\n",
      "================================================================================\n",
      "Loading GSE54564 data...\n",
      "Expression matrix: (48803, 42)\n",
      "Labels: {1: 21, 0: 21}\n",
      "BGX annotation: (49617, 28)\n",
      "Final shape: X=(42, 36157), y=(42,), genes=36157\n",
      "Raw data saved to 'output/Phase_2/raw_data.pkl'\n",
      "\n",
      "================================================================================\n",
      "STEP 2: PREPROCESSING\n",
      "================================================================================\n",
      "\n",
      "Preprocessing expression data...\n",
      "Input: (42, 36157), gene_names: 36157\n",
      "Genes: 36157 -> 36157 (removed 0)\n",
      "\n",
      "Verification:\n",
      "  X_processed shape: (42, 36157)\n",
      "  gene_names_filtered length: 36157\n",
      "  Match: True\n",
      "Processed data saved to 'output/Phase_2/processed_data.pkl'\n",
      "\n",
      "================================================================================\n",
      "DATA LOADING COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "data1 = \"../data/GSE54564_non-normalized.txt\"\n",
    "data2 = \"../data/GSE54564_series_matrix.txt\"\n",
    "data3 = \"../data/NCBI_Depression.bgx\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 1: LOADING DATA\")\n",
    "print(\"=\" * 80)\n",
    "X_raw, y, gene_names_raw, probe_mapping = load_gse54564_data(data1, data2, data3)\n",
    "\n",
    "with open('output/Phase_2/raw_data.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'X_raw': X_raw,\n",
    "        'y': y,\n",
    "        'gene_names_raw': gene_names_raw,\n",
    "        'probe_mapping': probe_mapping\n",
    "    }, f)\n",
    "print(\"Raw data saved to 'output/Phase_2/raw_data.pkl'\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 2: PREPROCESSING\")\n",
    "print(\"=\" * 80)\n",
    "X_processed, gene_names_filtered = preprocess_expression_data(X_raw, gene_names_raw)\n",
    "\n",
    "print(f\"\\nVerification:\")\n",
    "print(f\"  X_processed shape: {X_processed.shape}\")\n",
    "print(f\"  gene_names_filtered length: {len(gene_names_filtered) if gene_names_filtered else None}\")\n",
    "print(f\"  Match: {len(gene_names_filtered) == X_processed.shape[1] if gene_names_filtered else False}\")\n",
    "\n",
    "with open('output/Phase_2/processed_data.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'X_processed': X_processed,\n",
    "        'y': y,\n",
    "        'gene_names_filtered': gene_names_filtered\n",
    "    }, f)\n",
    "print(\"Processed data saved to 'output/Phase_2/processed_data.pkl'\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATA LOADING COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a142bc4-fa04-4634-b7b9-f50dc2b85632",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StableGeneSelector:\n",
    "    \"\"\"Robust feature selection combining multiple strategies with bootstrap stability\"\"\"\n",
    "    \n",
    "    def __init__(self, n_genes_to_select=50, n_bootstrap=100, stability_threshold=0.5, random_state=42):\n",
    "        self.n_genes_to_select = n_genes_to_select\n",
    "        self.n_bootstrap = n_bootstrap\n",
    "        self.stability_threshold = stability_threshold\n",
    "        self.random_state = random_state\n",
    "        self.selected_genes_ = None\n",
    "        self.gene_scores_ = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Identify stable genes using multiple selection strategies\"\"\"\n",
    "        np.random.seed(self.random_state)\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        l1_scores = np.zeros(n_features)\n",
    "        univariate_scores = np.zeros(n_features)\n",
    "        elastic_scores = np.zeros(n_features)\n",
    "        \n",
    "        print(f\"  Running {self.n_bootstrap} bootstrap iterations for stability selection...\")\n",
    "        \n",
    "        for i in tqdm(range(self.n_bootstrap), desc=f\"  StableGeneSelector (n={self.n_bootstrap})\", leave=False):\n",
    "            idx = resample(np.arange(n_samples), n_samples=int(0.7 * n_samples), random_state=i)\n",
    "            X_boot = X[idx]\n",
    "            y_boot = y[idx]\n",
    "            \n",
    "            scaler = StandardScaler()\n",
    "            X_scaled = scaler.fit_transform(X_boot)\n",
    "            \n",
    "            C_value = np.random.choice([0.01, 0.05, 0.1, 0.2])\n",
    "            try:\n",
    "                l1_model = LogisticRegression(penalty='l1', C=C_value, solver='liblinear',\n",
    "                                             max_iter=1000, random_state=i)\n",
    "                l1_model.fit(X_scaled, y_boot)\n",
    "                selected_l1 = np.abs(l1_model.coef_[0]) > 1e-6\n",
    "                l1_scores += selected_l1\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            selector = SelectKBest(f_classif, k=min(self.n_genes_to_select * 2, n_features))\n",
    "            selector.fit(X_boot, y_boot)\n",
    "            univariate_scores += selector.get_support()\n",
    "            \n",
    "            if n_samples > 20:\n",
    "                try:\n",
    "                    elastic = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.7,\n",
    "                                                C=0.1, max_iter=5000, random_state=i)\n",
    "                    elastic.fit(X_scaled, y_boot)\n",
    "                    selected_elastic = np.abs(elastic.coef_[0]) > 1e-6\n",
    "                    elastic_scores += selected_elastic\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        l1_scores = l1_scores / self.n_bootstrap\n",
    "        univariate_scores = univariate_scores / self.n_bootstrap\n",
    "        elastic_scores = elastic_scores / self.n_bootstrap\n",
    "        \n",
    "        if n_samples > 20:\n",
    "            combined_scores = 0.4 * l1_scores + 0.3 * univariate_scores + 0.3 * elastic_scores\n",
    "        else:\n",
    "            combined_scores = 0.6 * l1_scores + 0.4 * univariate_scores\n",
    "        \n",
    "        stable_mask = combined_scores >= self.stability_threshold\n",
    "        stable_indices = np.where(stable_mask)[0]\n",
    "        \n",
    "        if len(stable_indices) < self.n_genes_to_select:\n",
    "            stable_indices = np.argsort(combined_scores)[-self.n_genes_to_select:]\n",
    "        elif len(stable_indices) > self.n_genes_to_select * 2:\n",
    "            top_indices = np.argsort(combined_scores[stable_indices])[-self.n_genes_to_select:]\n",
    "            stable_indices = stable_indices[top_indices]\n",
    "        \n",
    "        self.selected_genes_ = stable_indices\n",
    "        self.gene_scores_ = combined_scores\n",
    "        \n",
    "        print(f\"  Selected {len(self.selected_genes_)} stable genes\")\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X[:, self.selected_genes_]\n",
    "    \n",
    "    def fit_transform(self, X, y):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25286514-da07-4aec-aeee-5c4612b1c5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_cv_univariate_svm(X, y, random_state=42):\n",
    "    \"\"\"Nested CV with univariate feature selection and RBF-SVM\"\"\"\n",
    "    \n",
    "    print(\"\\nAPPROACH 1: Nested CV with Univariate Feature Selection + RBF-SVM\")\n",
    "    \n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('selector', SelectKBest(f_classif)),\n",
    "        ('classifier', SVC(kernel='rbf', probability=True))\n",
    "    ])\n",
    "    \n",
    "    param_grid = {\n",
    "        'selector__k': [20, 50, 100, 200, 500],\n",
    "        'classifier__C': [0.01, 0.1, 1, 10, 100],\n",
    "        'classifier__gamma': ['scale', 0.001, 0.01, 0.1]\n",
    "    }\n",
    "    \n",
    "    outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "    inner_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    outer_scores = {'accuracy': [], 'roc_auc': [], 'mcc': [], 'best_params': []}\n",
    "    \n",
    "    n_splits = outer_cv.get_n_splits()\n",
    "    fold_iterator = tqdm(outer_cv.split(X, y), desc=\"  Outer Folds\", total=n_splits)\n",
    "    \n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(fold_iterator):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        grid_search = GridSearchCV(pipe, param_grid, cv=inner_cv,\n",
    "                                  scoring='roc_auc', n_jobs=-1, verbose=1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        best_model = grid_search.best_estimator_\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        mcc = matthews_corrcoef(y_test, y_pred)\n",
    "        \n",
    "        outer_scores['accuracy'].append(acc)\n",
    "        outer_scores['roc_auc'].append(auc)\n",
    "        outer_scores['mcc'].append(mcc)\n",
    "        outer_scores['best_params'].append(grid_search.best_params_)\n",
    "        \n",
    "        fold_iterator.set_postfix(auc=f\"{auc:.3f}\", mcc=f\"{mcc:.3f}\")\n",
    "    \n",
    "    print(f\"\\n  Mean Accuracy: {np.mean(outer_scores['accuracy']):.3f} ± {np.std(outer_scores['accuracy']):.3f}\")\n",
    "    print(f\"  Mean ROC-AUC: {np.mean(outer_scores['roc_auc']):.3f} ± {np.std(outer_scores['roc_auc']):.3f}\")\n",
    "    print(f\"  Mean MCC: {np.mean(outer_scores['mcc']):.3f} ± {np.std(outer_scores['mcc']):.3f}\")\n",
    "    \n",
    "    return outer_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74eb511f-76aa-4751-a470-c11c2480b7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elastic_net_feature_selection_pipeline(X, y, random_state=42):\n",
    "    \"\"\"L1 (Lasso) feature selection with non-linear classifier\"\"\"\n",
    "    \n",
    "    print(\"\\nAPPROACH 2: L1 (Lasso) Feature Selection + Non-linear Classifier\")\n",
    "    \n",
    "    l1_selector = LinearSVC(penalty='l1', dual=False, max_iter=5000, random_state=random_state)\n",
    "    \n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('feature_selector', SelectFromModel(l1_selector)),\n",
    "        ('classifier', SVC(kernel='rbf', probability=True))\n",
    "    ])\n",
    "    \n",
    "    param_grid = {\n",
    "        'feature_selector__estimator__C': [0.01, 0.1, 1.0],\n",
    "        'feature_selector__threshold': ['0.5*mean', 'mean', '1.25*mean'],\n",
    "        'classifier__C': [0.1, 1, 10],\n",
    "        'classifier__gamma': ['scale']\n",
    "    }\n",
    "    \n",
    "    outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "    inner_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    outer_scores = {'accuracy': [], 'roc_auc': [], 'mcc': [], 'n_features': []}\n",
    "    \n",
    "    n_splits = outer_cv.get_n_splits()\n",
    "    fold_iterator = tqdm(outer_cv.split(X, y), desc=\"  Outer Folds\", total=n_splits)\n",
    "    \n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(fold_iterator):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        grid_search = GridSearchCV(pipe, param_grid, cv=inner_cv,\n",
    "                                  scoring='roc_auc', n_jobs=-1, verbose=1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        best_model = grid_search.best_estimator_\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        n_features = np.sum(best_model.named_steps['feature_selector'].get_support())\n",
    "        \n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        mcc = matthews_corrcoef(y_test, y_pred)\n",
    "        \n",
    "        outer_scores['accuracy'].append(acc)\n",
    "        outer_scores['roc_auc'].append(auc)\n",
    "        outer_scores['mcc'].append(mcc)\n",
    "        outer_scores['n_features'].append(n_features)\n",
    "        \n",
    "        fold_iterator.set_postfix(auc=f\"{auc:.3f}\", n_feat=n_features)\n",
    "    \n",
    "    print(f\"\\n  Mean Accuracy: {np.mean(outer_scores['accuracy']):.3f} ± {np.std(outer_scores['accuracy']):.3f}\")\n",
    "    print(f\"  Mean ROC-AUC: {np.mean(outer_scores['roc_auc']):.3f} ± {np.std(outer_scores['roc_auc']):.3f}\")\n",
    "    print(f\"  Mean MCC: {np.mean(outer_scores['mcc']):.3f} ± {np.std(outer_scores['mcc']):.3f}\")\n",
    "    print(f\"  Mean Features: {np.mean(outer_scores['n_features']):.0f} ± {np.std(outer_scores['n_features']):.0f}\")\n",
    "    \n",
    "    return outer_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fe11b70-99b0-4703-bdb4-dd396986e3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stability_selection_pipeline(X, y, n_bootstrap=50, threshold=0.6, random_state=42):\n",
    "    \"\"\"Stability selection for robust feature identification\"\"\"\n",
    "    \n",
    "    print(\"\\nAPPROACH 3: Stability Selection + RBF-SVM\")\n",
    "    \n",
    "    selector = StableGeneSelector(n_genes_to_select=50, n_bootstrap=n_bootstrap,\n",
    "                                  stability_threshold=threshold, random_state=random_state)\n",
    "    X_selected = selector.fit_transform(X, y)\n",
    "    \n",
    "    print(f\"\\n  Evaluating classifier on {X_selected.shape[1]} stable features...\")\n",
    "    \n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', SVC(kernel='rbf', probability=True))\n",
    "    ])\n",
    "    \n",
    "    param_grid = {\n",
    "        'classifier__C': [0.01, 0.1, 1, 10, 100],\n",
    "        'classifier__gamma': ['scale', 0.001, 0.01, 0.1]\n",
    "    }\n",
    "    \n",
    "    outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "    inner_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    outer_scores = {'accuracy': [], 'roc_auc': [], 'mcc': []}\n",
    "    \n",
    "    n_splits = outer_cv.get_n_splits()\n",
    "    fold_iterator = tqdm(outer_cv.split(X_selected, y), desc=\"  Evaluation Folds\", total=n_splits)\n",
    "    \n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(fold_iterator):\n",
    "        X_train, X_test = X_selected[train_idx], X_selected[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        grid_search = GridSearchCV(pipe, param_grid, cv=inner_cv,\n",
    "                                  scoring='roc_auc', n_jobs=-1, verbose=1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        best_model = grid_search.best_estimator_\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        mcc = matthews_corrcoef(y_test, y_pred)\n",
    "        \n",
    "        outer_scores['accuracy'].append(acc)\n",
    "        outer_scores['roc_auc'].append(auc)\n",
    "        outer_scores['mcc'].append(mcc)\n",
    "        \n",
    "        fold_iterator.set_postfix(auc=f\"{auc:.3f}\", mcc=f\"{mcc:.3f}\")\n",
    "    \n",
    "    print(f\"\\n  Mean Accuracy: {np.mean(outer_scores['accuracy']):.3f} ± {np.std(outer_scores['accuracy']):.3f}\")\n",
    "    print(f\"  Mean ROC-AUC: {np.mean(outer_scores['roc_auc']):.3f} ± {np.std(outer_scores['roc_auc']):.3f}\")\n",
    "    print(f\"  Mean MCC: {np.mean(outer_scores['mcc']):.3f} ± {np.std(outer_scores['mcc']):.3f}\")\n",
    "    \n",
    "    return outer_scores, selector.selected_genes_, selector.gene_scores_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fa64bd9-ab75-472e-911b-eb14533ec92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutual_information_rfe_pipeline(X, y, random_state=42):\n",
    "    \"\"\"Two-stage feature selection: MI pre-filter + RFE fine-tuning\"\"\"\n",
    "    \n",
    "    print(\"\\nAPPROACH 4: Mutual Information + RFE Pipeline\")\n",
    "    \n",
    "    class MIRFEPipeline:\n",
    "        def __init__(self, n_mi_features=1000, n_rfe_features=50):\n",
    "            self.n_mi_features = n_mi_features\n",
    "            self.n_rfe_features = n_rfe_features\n",
    "            self.scaler = StandardScaler()\n",
    "            self.mi_selector = SelectKBest(mutual_info_classif, k=n_mi_features)\n",
    "            self.rfe_selector = None\n",
    "            self.classifier = SVC(kernel='rbf', probability=True)\n",
    "            \n",
    "        def fit(self, X, y):\n",
    "            X_scaled = self.scaler.fit_transform(X)\n",
    "            X_mi = self.mi_selector.fit_transform(X_scaled, y)\n",
    "            \n",
    "            if X_mi.shape[1] > self.n_rfe_features:\n",
    "                self.rfe_selector = RFE(LinearSVC(max_iter=5000, random_state=42),\n",
    "                                       n_features_to_select=self.n_rfe_features, step=0.1)\n",
    "                X_rfe = self.rfe_selector.fit_transform(X_mi, y)\n",
    "            else:\n",
    "                X_rfe = X_mi\n",
    "                \n",
    "            self.classifier.fit(X_rfe, y)\n",
    "            return self\n",
    "            \n",
    "        def predict(self, X):\n",
    "            X_scaled = self.scaler.transform(X)\n",
    "            X_mi = self.mi_selector.transform(X_scaled)\n",
    "            if self.rfe_selector is not None:\n",
    "                X_rfe = self.rfe_selector.transform(X_mi)\n",
    "            else:\n",
    "                X_rfe = X_mi\n",
    "            return self.classifier.predict(X_rfe)\n",
    "            \n",
    "        def predict_proba(self, X):\n",
    "            X_scaled = self.scaler.transform(X)\n",
    "            X_mi = self.mi_selector.transform(X_scaled)\n",
    "            if self.rfe_selector is not None:\n",
    "                X_rfe = self.rfe_selector.transform(X_mi)\n",
    "            else:\n",
    "                X_rfe = X_mi\n",
    "            return self.classifier.predict_proba(X_rfe)\n",
    "    \n",
    "    configs = [(1000, 50), (500, 30), (2000, 100)]\n",
    "    best_config = None\n",
    "    best_score = -1\n",
    "    \n",
    "    print(\"  Testing feature configurations...\")\n",
    "    \n",
    "    for n_mi, n_rfe in tqdm(configs, desc=\"  Config Testing\"):\n",
    "        pipeline = MIRFEPipeline(n_mi_features=min(n_mi, X.shape[1]), n_rfe_features=n_rfe)\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "        scores = []\n",
    "        \n",
    "        for train_idx, test_idx in cv.split(X, y):\n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "            \n",
    "            pipeline.fit(X_train, y_train)\n",
    "            y_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "            scores.append(roc_auc_score(y_test, y_pred_proba))\n",
    "        \n",
    "        mean_score = np.mean(scores)\n",
    "        print(f\"    MI={n_mi}, RFE={n_rfe}: ROC-AUC={mean_score:.3f}\")\n",
    "        \n",
    "        if mean_score > best_score:\n",
    "            best_score = mean_score\n",
    "            best_config = (n_mi, n_rfe)\n",
    "    \n",
    "    print(f\"\\n  Best configuration: MI={best_config[0]}, RFE={best_config[1]}\")\n",
    "    \n",
    "    pipeline = MIRFEPipeline(n_mi_features=min(best_config[0], X.shape[1]), n_rfe_features=best_config[1])\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    outer_scores = {'accuracy': [], 'roc_auc': [], 'mcc': []}\n",
    "    \n",
    "    n_splits = cv.get_n_splits()\n",
    "    fold_iterator = tqdm(cv.split(X, y), desc=\"  Final Evaluation\", total=n_splits)\n",
    "    \n",
    "    for train_idx, test_idx in fold_iterator:\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "        y_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        mcc = matthews_corrcoef(y_test, y_pred)\n",
    "        \n",
    "        outer_scores['accuracy'].append(acc)\n",
    "        outer_scores['roc_auc'].append(auc)\n",
    "        outer_scores['mcc'].append(mcc)\n",
    "        \n",
    "        fold_iterator.set_postfix(auc=f\"{auc:.3f}\")\n",
    "    \n",
    "    print(f\"\\n  Mean Accuracy: {np.mean(outer_scores['accuracy']):.3f} ± {np.std(outer_scores['accuracy']):.3f}\")\n",
    "    print(f\"  Mean ROC-AUC: {np.mean(outer_scores['roc_auc']):.3f} ± {np.std(outer_scores['roc_auc']):.3f}\")\n",
    "    print(f\"  Mean MCC: {np.mean(outer_scores['mcc']):.3f} ± {np.std(outer_scores['mcc']):.3f}\")\n",
    "    \n",
    "    return outer_scores, best_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68229392-e301-4ddf-a89d-5a9158475d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_feature_bagging(X, y, n_estimators=50, feature_fraction=0.1, random_state=42):\n",
    "    \"\"\"Ensemble of models trained on random feature subsets\"\"\"\n",
    "    \n",
    "    print(\"\\nADVANCED: Feature Bagging Ensemble\")\n",
    "    \n",
    "    np.random.seed(random_state)\n",
    "    n_features = X.shape[1]\n",
    "    n_features_per_model = int(n_features * feature_fraction)\n",
    "    \n",
    "    print(f\"  Training {n_estimators} models with {n_features_per_model} features each...\")\n",
    "    \n",
    "    feature_subsets = []\n",
    "    for i in range(n_estimators):\n",
    "        feature_indices = np.random.choice(n_features, n_features_per_model, replace=False)\n",
    "        feature_subsets.append(feature_indices)\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "    scores = {'accuracy': [], 'roc_auc': [], 'mcc': []}\n",
    "    \n",
    "    n_splits = cv.get_n_splits()\n",
    "    fold_iterator = tqdm(cv.split(X, y), desc=\"  Outer Folds\", total=n_splits)\n",
    "    \n",
    "    for train_idx, test_idx in fold_iterator:\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        fold_models = []\n",
    "        \n",
    "        for feature_indices in tqdm(feature_subsets, desc=\"    Training Ensemble\", leave=False):\n",
    "            X_subset = X_train[:, feature_indices]\n",
    "            scaler = StandardScaler()\n",
    "            X_scaled = scaler.fit_transform(X_subset)\n",
    "            \n",
    "            model = SVC(kernel='rbf', probability=True, C=1.0, gamma='scale')\n",
    "            model.fit(X_scaled, y_train)\n",
    "            \n",
    "            fold_models.append({'model': model, 'scaler': scaler, 'features': feature_indices})\n",
    "        \n",
    "        predictions_proba = []\n",
    "        for model_info in fold_models:\n",
    "            X_test_subset = X_test[:, model_info['features']]\n",
    "            X_test_scaled = model_info['scaler'].transform(X_test_subset)\n",
    "            pred_proba = model_info['model'].predict_proba(X_test_scaled)[:, 1]\n",
    "            predictions_proba.append(pred_proba)\n",
    "        \n",
    "        y_pred_proba = np.mean(predictions_proba, axis=0)\n",
    "        y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "        \n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        mcc = matthews_corrcoef(y_test, y_pred)\n",
    "        \n",
    "        scores['accuracy'].append(acc)\n",
    "        scores['roc_auc'].append(auc)\n",
    "        scores['mcc'].append(mcc)\n",
    "        \n",
    "        fold_iterator.set_postfix(auc=f\"{auc:.3f}\")\n",
    "    \n",
    "    print(f\"\\n  Mean Accuracy: {np.mean(scores['accuracy']):.3f} ± {np.std(scores['accuracy']):.3f}\")\n",
    "    print(f\"  Mean ROC-AUC: {np.mean(scores['roc_auc']):.3f} ± {np.std(scores['roc_auc']):.3f}\")\n",
    "    print(f\"  Mean MCC: {np.mean(scores['mcc']):.3f} ± {np.std(scores['mcc']):.3f}\")\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a55a57cd-4692-424c-bd40-7e76c0dae7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_nested_cv_pipeline(X, y, n_genes=50, random_state=42):\n",
    "    \"\"\"Nested Leave-One-Out CV with stability selection\"\"\"\n",
    "    \n",
    "    print(\"\\nOPTIMAL PIPELINE: Nested Leave-One-Out CV with Stability Selection\")\n",
    "    \n",
    "    outer_cv = LeaveOneOut()\n",
    "    inner_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    results = {\n",
    "        'predictions': [],\n",
    "        'probabilities': [],\n",
    "        'true_labels': [],\n",
    "        'selected_genes_per_fold': []\n",
    "    }\n",
    "    \n",
    "    print(f\"  Running Leave-One-Out CV ({X.shape[0]} iterations)...\")\n",
    "    \n",
    "    fold_iterator = tqdm(outer_cv.split(X, y), desc=\"  LOO-CV Folds\", total=X.shape[0])\n",
    "    \n",
    "    param_combos = list(itertools.product(\n",
    "        [0.01, 0.1, 1.0, 10.0],\n",
    "        ['scale', 0.01, 0.1]\n",
    "    ))\n",
    "    \n",
    "    for i, (train_idx, test_idx) in enumerate(fold_iterator):\n",
    "        X_train_outer, X_test_outer = X[train_idx], X[test_idx]\n",
    "        y_train_outer, y_test_outer = y[train_idx], y[test_idx]\n",
    "        \n",
    "        selector = StableGeneSelector(n_genes_to_select=n_genes, n_bootstrap=5,\n",
    "                                     stability_threshold=0.3, random_state=i)\n",
    "        X_train_selected = selector.fit_transform(X_train_outer, y_train_outer)\n",
    "        X_test_selected = selector.transform(X_test_outer)\n",
    "        \n",
    "        best_score = -1\n",
    "        best_params = {'C': 1.0, 'gamma': 'scale'}\n",
    "        \n",
    "        param_iterator = tqdm(param_combos, desc=\"    Hyperparameter Tuning\", leave=False)\n",
    "        \n",
    "        for C, gamma in param_iterator:\n",
    "            scores = []\n",
    "            \n",
    "            for train_inner, val_inner in inner_cv.split(X_train_selected, y_train_outer):\n",
    "                X_train_inner = X_train_selected[train_inner]\n",
    "                y_train_inner = y_train_outer[train_inner]\n",
    "                X_val_inner = X_train_selected[val_inner]\n",
    "                y_val_inner = y_train_outer[val_inner]\n",
    "                \n",
    "                scaler = StandardScaler()\n",
    "                X_train_scaled = scaler.fit_transform(X_train_inner)\n",
    "                X_val_scaled = scaler.transform(X_val_inner)\n",
    "                \n",
    "                clf = SVC(kernel='rbf', C=C, gamma=gamma, probability=True)\n",
    "                clf.fit(X_train_scaled, y_train_inner)\n",
    "                \n",
    "                val_prob = clf.predict_proba(X_val_scaled)[:, 1]\n",
    "                score = roc_auc_score(y_val_inner, val_prob)\n",
    "                scores.append(score)\n",
    "            \n",
    "            mean_score = np.mean(scores)\n",
    "            if mean_score > best_score:\n",
    "                best_score = mean_score\n",
    "                best_params = {'C': C, 'gamma': gamma}\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train_selected)\n",
    "        X_test_scaled = scaler.transform(X_test_selected)\n",
    "        \n",
    "        final_clf = SVC(kernel='rbf', C=best_params['C'], gamma=best_params['gamma'], probability=True)\n",
    "        final_clf.fit(X_train_scaled, y_train_outer)\n",
    "        \n",
    "        pred = final_clf.predict(X_test_scaled)[0]\n",
    "        prob = final_clf.predict_proba(X_test_scaled)[0, 1]\n",
    "        \n",
    "        results['predictions'].append(pred)\n",
    "        results['probabilities'].append(prob)\n",
    "        results['true_labels'].append(y_test_outer[0])\n",
    "        results['selected_genes_per_fold'].append(selector.selected_genes_)\n",
    "    \n",
    "    predictions = np.array(results['predictions'])\n",
    "    probabilities = np.array(results['probabilities'])\n",
    "    true_labels = np.array(results['true_labels'])\n",
    "    \n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    auc = roc_auc_score(true_labels, probabilities)\n",
    "    mcc = matthews_corrcoef(true_labels, predictions)\n",
    "    cm = confusion_matrix(true_labels, predictions)\n",
    "    \n",
    "    print(f\"\\n  Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"  ROC-AUC: {auc:.3f}\")\n",
    "    print(f\"  MCC: {mcc:.3f}\")\n",
    "    print(f\"  Confusion Matrix: TN={cm[0,0]}, FP={cm[0,1]}, FN={cm[1,0]}, TP={cm[1,1]}\")\n",
    "    print(f\"  Sensitivity: {cm[1,1]/(cm[1,0]+cm[1,1]):.3f}\")\n",
    "    print(f\"  Specificity: {cm[0,0]/(cm[0,0]+cm[0,1]):.3f}\")\n",
    "    \n",
    "    gene_selection_counts = np.zeros(X.shape[1])\n",
    "    for genes in results['selected_genes_per_fold']:\n",
    "        gene_selection_counts[genes] += 1\n",
    "    \n",
    "    consistent_genes = np.where(gene_selection_counts > (X.shape[0] * 0.5))[0]\n",
    "    print(f\"  Consistently selected genes (>50% folds): {len(consistent_genes)}\")\n",
    "    \n",
    "    results['metrics'] = {\n",
    "        'accuracy': accuracy,\n",
    "        'auc': auc,\n",
    "        'mcc': mcc,\n",
    "        'confusion_matrix': cm,\n",
    "        'consistent_genes': consistent_genes,\n",
    "        'gene_selection_frequency': gene_selection_counts / X.shape[0]\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1fa58b7a-9e48-44e4-8731-e92993a9eea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data from memory\n",
      "\n",
      "APPROACH 1: Univariate Feature Selection + RBF-SVM\n",
      "Dataset: 42 samples × 36157 genes\n",
      "Method: ANOVA F-test + Nested CV with hyperparameter tuning\n",
      "\n",
      "APPROACH 1: Nested CV with Univariate Feature Selection + RBF-SVM\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82eadb81aa2e482cb9efce76f503b6cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Outer Folds:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "\n",
      "  Mean Accuracy: 0.525 ± 0.061\n",
      "  Mean ROC-AUC: 0.685 ± 0.074\n",
      "  Mean MCC: 0.072 ± 0.101\n",
      "\n",
      "APPROACH 1 RESULTS SUMMARY\n",
      "Overall Performance:\n",
      "  Accuracy:     0.525 ± 0.061\n",
      "  ROC-AUC:      0.685 ± 0.074\n",
      "  MCC:          0.072 ± 0.101\n",
      "\n",
      "Per-Fold Breakdown:\n",
      "  Fold 1: Acc=0.556 | AUC=0.600 | MCC=0.100\n",
      "  Fold 2: Acc=0.444 | AUC=0.700 | MCC=0.000\n",
      "  Fold 3: Acc=0.500 | AUC=0.688 | MCC=0.000\n",
      "  Fold 4: Acc=0.500 | AUC=0.625 | MCC=0.000\n",
      "  Fold 5: Acc=0.625 | AUC=0.812 | MCC=0.258\n",
      "\n",
      "Hyperparameter Analysis:\n",
      "  Features (k):  68 ± 67 (range: 20-200)\n",
      "  C values:      Most common = 100\n",
      "  Gamma values:  Most common = 0.001\n",
      "\n",
      "Performance Interpretation:\n",
      "  FAIR performance (0.6 <= AUC < 0.7)\n",
      "\n",
      "Results saved:\n",
      "  output/Phase_2/results_approach1.pkl\n",
      "  output/Phase_2/approach1_detailed.csv\n",
      "  output/Phase_2/approach1_summary.csv\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    X_processed\n",
    "    print(\"Using data from memory\")\n",
    "except NameError:\n",
    "    print(\"Loading from disk...\")\n",
    "    with open('output/Phase_2/processed_data.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        X_processed = data['X_processed']\n",
    "        y = data['y']\n",
    "        gene_names_filtered = data['gene_names_filtered']\n",
    "\n",
    "print(\"\\nAPPROACH 1: Univariate Feature Selection + RBF-SVM\")\n",
    "print(f\"Dataset: {X_processed.shape[0]} samples × {X_processed.shape[1]} genes\")\n",
    "print(f\"Method: ANOVA F-test + Nested CV with hyperparameter tuning\")\n",
    "\n",
    "results_univariate = nested_cv_univariate_svm(X_processed, y)\n",
    "\n",
    "with open('output/Phase_2/results_approach1.pkl', 'wb') as f:\n",
    "    pickle.dump(results_univariate, f)\n",
    "\n",
    "print(\"\\nAPPROACH 1 RESULTS SUMMARY\")\n",
    "print(\"Overall Performance:\")\n",
    "print(f\"  Accuracy:     {np.mean(results_univariate['accuracy']):.3f} ± {np.std(results_univariate['accuracy']):.3f}\")\n",
    "print(f\"  ROC-AUC:      {np.mean(results_univariate['roc_auc']):.3f} ± {np.std(results_univariate['roc_auc']):.3f}\")\n",
    "print(f\"  MCC:          {np.mean(results_univariate['mcc']):.3f} ± {np.std(results_univariate['mcc']):.3f}\")\n",
    "\n",
    "print(\"\\nPer-Fold Breakdown:\")\n",
    "for fold_idx in range(len(results_univariate['accuracy'])):\n",
    "    print(f\"  Fold {fold_idx+1}: Acc={results_univariate['accuracy'][fold_idx]:.3f} | \"\n",
    "          f\"AUC={results_univariate['roc_auc'][fold_idx]:.3f} | \"\n",
    "          f\"MCC={results_univariate['mcc'][fold_idx]:.3f}\")\n",
    "\n",
    "print(\"\\nHyperparameter Analysis:\")\n",
    "param_k = [p['selector__k'] for p in results_univariate['best_params']]\n",
    "param_C = [p['classifier__C'] for p in results_univariate['best_params']]\n",
    "param_gamma = [p['classifier__gamma'] for p in results_univariate['best_params']]\n",
    "\n",
    "print(f\"  Features (k):  {np.mean(param_k):.0f} ± {np.std(param_k):.0f} (range: {min(param_k)}-{max(param_k)})\")\n",
    "print(f\"  C values:      Most common = {Counter(param_C).most_common(1)[0][0]}\")\n",
    "print(f\"  Gamma values:  Most common = {Counter(param_gamma).most_common(1)[0][0]}\")\n",
    "\n",
    "print(\"\\nPerformance Interpretation:\")\n",
    "mean_auc = np.mean(results_univariate['roc_auc'])\n",
    "if mean_auc >= 0.8:\n",
    "    print(\"  EXCELLENT performance (AUC >= 0.8)\")\n",
    "elif mean_auc >= 0.7:\n",
    "    print(\"  GOOD performance (0.7 <= AUC < 0.8)\")\n",
    "elif mean_auc >= 0.6:\n",
    "    print(\"  FAIR performance (0.6 <= AUC < 0.7)\")\n",
    "else:\n",
    "    print(\"  POOR performance (AUC < 0.6)\")\n",
    "\n",
    "approach1_detailed = pd.DataFrame({\n",
    "    'Fold': range(1, len(results_univariate['accuracy']) + 1),\n",
    "    'Accuracy': results_univariate['accuracy'],\n",
    "    'ROC_AUC': results_univariate['roc_auc'],\n",
    "    'MCC': results_univariate['mcc'],\n",
    "    'Best_k': param_k,\n",
    "    'Best_C': param_C,\n",
    "    'Best_gamma': [str(g) for g in param_gamma]\n",
    "})\n",
    "approach1_detailed.to_csv('output/Phase_2/approach1_detailed.csv', index=False)\n",
    "\n",
    "approach1_summary = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'ROC-AUC', 'MCC', 'Avg_Features'],\n",
    "    'Mean': [np.mean(results_univariate['accuracy']), np.mean(results_univariate['roc_auc']),\n",
    "             np.mean(results_univariate['mcc']), np.mean(param_k)],\n",
    "    'Std': [np.std(results_univariate['accuracy']), np.std(results_univariate['roc_auc']),\n",
    "            np.std(results_univariate['mcc']), np.std(param_k)],\n",
    "    'Min': [np.min(results_univariate['accuracy']), np.min(results_univariate['roc_auc']),\n",
    "            np.min(results_univariate['mcc']), min(param_k)],\n",
    "    'Max': [np.max(results_univariate['accuracy']), np.max(results_univariate['roc_auc']),\n",
    "            np.max(results_univariate['mcc']), max(param_k)]\n",
    "})\n",
    "approach1_summary.to_csv('output/Phase_2/approach1_summary.csv', index=False)\n",
    "\n",
    "print(\"\\nResults saved:\")\n",
    "print(\"  output/Phase_2/results_approach1.pkl\")\n",
    "print(\"  output/Phase_2/approach1_detailed.csv\")\n",
    "print(\"  output/Phase_2/approach1_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dba86a57-931b-4912-9bb0-04fb9d1678d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data from memory\n",
      "\n",
      "APPROACH 2: L1 (Lasso) Feature Selection + RBF-SVM\n",
      "Dataset: 42 samples × 36157 genes\n",
      "Method: LinearSVC L1 penalty + Nested CV\n",
      "\n",
      "APPROACH 2: L1 (Lasso) Feature Selection + Non-linear Classifier\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de4c17b82c6a4aef8ad9bbf03564344d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Outer Folds:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "\n",
      "  Mean Accuracy: 0.578 ± 0.118\n",
      "  Mean ROC-AUC: 0.555 ± 0.166\n",
      "  Mean MCC: 0.243 ± 0.223\n",
      "  Mean Features: 7256 ± 14451\n",
      "\n",
      "APPROACH 2 RESULTS SUMMARY\n",
      "Overall Performance:\n",
      "  Accuracy:     0.578 ± 0.118\n",
      "  ROC-AUC:      0.555 ± 0.166\n",
      "  MCC:          0.243 ± 0.223\n",
      "  Features:     7256 ± 14451\n",
      "\n",
      "Per-Fold Breakdown:\n",
      "  Fold 1: Acc=0.444 | AUC=0.450 | MCC=0.000 | Features=36157\n",
      "  Fold 2: Acc=0.444 | AUC=0.450 | MCC=0.000 | Features=28\n",
      "  Fold 3: Acc=0.750 | AUC=0.688 | MCC=0.577 | Features=34\n",
      "  Fold 4: Acc=0.625 | AUC=0.812 | MCC=0.258 | Features=32\n",
      "  Fold 5: Acc=0.625 | AUC=0.375 | MCC=0.378 | Features=29\n",
      "\n",
      "Feature Selection Analysis:\n",
      "  Min features:  28\n",
      "  Max features:  36157\n",
      "  Median:        32\n",
      "  Consistency:   199.15% (CV)\n",
      "\n",
      "Performance Interpretation:\n",
      "  POOR performance (AUC < 0.6)\n",
      "\n",
      "Results saved:\n",
      "  output/Phase_2/results_approach2.pkl\n",
      "  output/Phase_2/approach2_detailed.csv\n",
      "  output/Phase_2/approach2_summary.csv\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    X_processed\n",
    "    print(\"Using data from memory\")\n",
    "except NameError:\n",
    "    print(\"Loading from disk...\")\n",
    "    with open('output/Phase_2/processed_data.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        X_processed = data['X_processed']\n",
    "        y = data['y']\n",
    "        gene_names_filtered = data['gene_names_filtered']\n",
    "\n",
    "print(\"\\nAPPROACH 2: L1 (Lasso) Feature Selection + RBF-SVM\")\n",
    "print(f\"Dataset: {X_processed.shape[0]} samples × {X_processed.shape[1]} genes\")\n",
    "print(f\"Method: LinearSVC L1 penalty + Nested CV\")\n",
    "\n",
    "results_l1 = elastic_net_feature_selection_pipeline(X_processed, y)\n",
    "\n",
    "with open('output/Phase_2/results_approach2.pkl', 'wb') as f:\n",
    "    pickle.dump(results_l1, f)\n",
    "\n",
    "print(\"\\nAPPROACH 2 RESULTS SUMMARY\")\n",
    "print(\"Overall Performance:\")\n",
    "print(f\"  Accuracy:     {np.mean(results_l1['accuracy']):.3f} ± {np.std(results_l1['accuracy']):.3f}\")\n",
    "print(f\"  ROC-AUC:      {np.mean(results_l1['roc_auc']):.3f} ± {np.std(results_l1['roc_auc']):.3f}\")\n",
    "print(f\"  MCC:          {np.mean(results_l1['mcc']):.3f} ± {np.std(results_l1['mcc']):.3f}\")\n",
    "print(f\"  Features:     {np.mean(results_l1['n_features']):.0f} ± {np.std(results_l1['n_features']):.0f}\")\n",
    "\n",
    "print(\"\\nPer-Fold Breakdown:\")\n",
    "for fold_idx in range(len(results_l1['accuracy'])):\n",
    "    print(f\"  Fold {fold_idx+1}: Acc={results_l1['accuracy'][fold_idx]:.3f} | \"\n",
    "          f\"AUC={results_l1['roc_auc'][fold_idx]:.3f} | \"\n",
    "          f\"MCC={results_l1['mcc'][fold_idx]:.3f} | \"\n",
    "          f\"Features={results_l1['n_features'][fold_idx]}\")\n",
    "\n",
    "print(\"\\nFeature Selection Analysis:\")\n",
    "print(f\"  Min features:  {min(results_l1['n_features'])}\")\n",
    "print(f\"  Max features:  {max(results_l1['n_features'])}\")\n",
    "print(f\"  Median:        {np.median(results_l1['n_features']):.0f}\")\n",
    "print(f\"  Consistency:   {np.std(results_l1['n_features'])/np.mean(results_l1['n_features']):.2%} (CV)\")\n",
    "\n",
    "print(\"\\nPerformance Interpretation:\")\n",
    "mean_auc = np.mean(results_l1['roc_auc'])\n",
    "if mean_auc >= 0.8:\n",
    "    print(\"  EXCELLENT performance (AUC >= 0.8)\")\n",
    "elif mean_auc >= 0.7:\n",
    "    print(\"  GOOD performance (0.7 <= AUC < 0.8)\")\n",
    "elif mean_auc >= 0.6:\n",
    "    print(\"  FAIR performance (0.6 <= AUC < 0.7)\")\n",
    "else:\n",
    "    print(\"  POOR performance (AUC < 0.6)\")\n",
    "\n",
    "approach2_detailed = pd.DataFrame({\n",
    "    'Fold': range(1, len(results_l1['accuracy']) + 1),\n",
    "    'Accuracy': results_l1['accuracy'],\n",
    "    'ROC_AUC': results_l1['roc_auc'],\n",
    "    'MCC': results_l1['mcc'],\n",
    "    'Num_Features': results_l1['n_features']\n",
    "})\n",
    "approach2_detailed.to_csv('output/Phase_2/approach2_detailed.csv', index=False)\n",
    "\n",
    "approach2_summary = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'ROC-AUC', 'MCC', 'Num_Features'],\n",
    "    'Mean': [np.mean(results_l1['accuracy']), np.mean(results_l1['roc_auc']),\n",
    "             np.mean(results_l1['mcc']), np.mean(results_l1['n_features'])],\n",
    "    'Std': [np.std(results_l1['accuracy']), np.std(results_l1['roc_auc']),\n",
    "            np.std(results_l1['mcc']), np.std(results_l1['n_features'])],\n",
    "    'Min': [np.min(results_l1['accuracy']), np.min(results_l1['roc_auc']),\n",
    "            np.min(results_l1['mcc']), min(results_l1['n_features'])],\n",
    "    'Max': [np.max(results_l1['accuracy']), np.max(results_l1['roc_auc']),\n",
    "            np.max(results_l1['mcc']), max(results_l1['n_features'])]\n",
    "})\n",
    "approach2_summary.to_csv('output/Phase_2/approach2_summary.csv', index=False)\n",
    "\n",
    "print(\"\\nResults saved:\")\n",
    "print(\"  output/Phase_2/results_approach2.pkl\")\n",
    "print(\"  output/Phase_2/approach2_detailed.csv\")\n",
    "print(\"  output/Phase_2/approach2_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25a0710a-b3c0-4cad-89fb-7fed664fc1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data from memory\n",
      "\n",
      "APPROACH 3: Stability Selection + RBF-SVM\n",
      "Dataset: 42 samples × 36157 genes\n",
      "Method: Bootstrap stability selection (50 iterations)\n",
      "\n",
      "APPROACH 3: Stability Selection + RBF-SVM\n",
      "  Running 50 bootstrap iterations for stability selection...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29082c37e9d04bbb84ce103f91d4e194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  StableGeneSelector (n=50):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Selected 50 stable genes\n",
      "\n",
      "  Evaluating classifier on 50 stable features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1c3e5c13c724fd3b2000a2bb54338c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Evaluation Folds:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "\n",
      "  Mean Accuracy: 0.778 ± 0.272\n",
      "  Mean ROC-AUC: 0.200 ± 0.245\n",
      "  Mean MCC: 0.600 ± 0.490\n",
      "\n",
      "APPROACH 3 RESULTS SUMMARY\n",
      "Overall Performance:\n",
      "  Accuracy:     0.778 ± 0.272\n",
      "  ROC-AUC:      0.200 ± 0.245\n",
      "  MCC:          0.600 ± 0.490\n",
      "  Stable Genes: 50\n",
      "\n",
      "Per-Fold Breakdown:\n",
      "  Fold 1: Acc=0.444 | AUC=0.000 | MCC=0.000\n",
      "  Fold 2: Acc=0.444 | AUC=0.000 | MCC=0.000\n",
      "  Fold 3: Acc=1.000 | AUC=0.500 | MCC=1.000\n",
      "  Fold 4: Acc=1.000 | AUC=0.500 | MCC=1.000\n",
      "  Fold 5: Acc=1.000 | AUC=0.000 | MCC=1.000\n",
      "\n",
      "Stable Genes Analysis:\n",
      "  Total selected: 50\n",
      "  Stability scores - Min: 0.060, Max: 0.232\n",
      "\n",
      "  Top 10 Most Stable Genes:\n",
      "    LOC284948: 0.232\n",
      "    NTRK1: 0.170\n",
      "    PPAP2C: 0.154\n",
      "    LOC648517: 0.138\n",
      "    WDR63: 0.122\n",
      "    WTAP: 0.114\n",
      "    LOC653513: 0.110\n",
      "    POGZ: 0.104\n",
      "    LOC342931: 0.096\n",
      "    PCDH24: 0.096\n",
      "  Full gene list saved to 'output/Phase_2/approach3_stable_genes.csv'\n",
      "\n",
      "Performance Interpretation:\n",
      "  POOR performance (AUC < 0.6)\n",
      "\n",
      "Results saved:\n",
      "  output/Phase_2/results_approach3.pkl\n",
      "  output/Phase_2/approach3_detailed.csv\n",
      "  output/Phase_2/approach3_summary.csv\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    X_processed\n",
    "    print(\"Using data from memory\")\n",
    "except NameError:\n",
    "    print(\"Loading from disk...\")\n",
    "    with open('output/Phase_2/processed_data.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        X_processed = data['X_processed']\n",
    "        y = data['y']\n",
    "        gene_names_filtered = data['gene_names_filtered']\n",
    "\n",
    "print(\"\\nAPPROACH 3: Stability Selection + RBF-SVM\")\n",
    "print(f\"Dataset: {X_processed.shape[0]} samples × {X_processed.shape[1]} genes\")\n",
    "print(f\"Method: Bootstrap stability selection (50 iterations)\")\n",
    "\n",
    "results_stability, stable_gene_indices, gene_scores = stability_selection_pipeline(\n",
    "    X_processed, y, n_bootstrap=50, threshold=0.6\n",
    ")\n",
    "\n",
    "with open('output/Phase_2/results_approach3.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'results': results_stability,\n",
    "        'stable_gene_indices': stable_gene_indices,\n",
    "        'gene_scores': gene_scores\n",
    "    }, f)\n",
    "\n",
    "print(\"\\nAPPROACH 3 RESULTS SUMMARY\")\n",
    "print(\"Overall Performance:\")\n",
    "print(f\"  Accuracy:     {np.mean(results_stability['accuracy']):.3f} ± {np.std(results_stability['accuracy']):.3f}\")\n",
    "print(f\"  ROC-AUC:      {np.mean(results_stability['roc_auc']):.3f} ± {np.std(results_stability['roc_auc']):.3f}\")\n",
    "print(f\"  MCC:          {np.mean(results_stability['mcc']):.3f} ± {np.std(results_stability['mcc']):.3f}\")\n",
    "print(f\"  Stable Genes: {len(stable_gene_indices)}\")\n",
    "\n",
    "print(\"\\nPer-Fold Breakdown:\")\n",
    "for fold_idx in range(len(results_stability['accuracy'])):\n",
    "    print(f\"  Fold {fold_idx+1}: Acc={results_stability['accuracy'][fold_idx]:.3f} | \"\n",
    "          f\"AUC={results_stability['roc_auc'][fold_idx]:.3f} | \"\n",
    "          f\"MCC={results_stability['mcc'][fold_idx]:.3f}\")\n",
    "\n",
    "print(\"\\nStable Genes Analysis:\")\n",
    "print(f\"  Total selected: {len(stable_gene_indices)}\")\n",
    "print(f\"  Stability scores - Min: {gene_scores[stable_gene_indices].min():.3f}, Max: {gene_scores[stable_gene_indices].max():.3f}\")\n",
    "\n",
    "if gene_names_filtered and len(stable_gene_indices) > 0:\n",
    "    max_idx = max(stable_gene_indices)\n",
    "    if max_idx < len(gene_names_filtered):\n",
    "        stable_genes_df = pd.DataFrame({\n",
    "            'gene': [gene_names_filtered[i] for i in stable_gene_indices],\n",
    "            'stability_score': gene_scores[stable_gene_indices]\n",
    "        }).sort_values('stability_score', ascending=False)\n",
    "        \n",
    "        print(\"\\n  Top 10 Most Stable Genes:\")\n",
    "        for idx, row in stable_genes_df.head(10).iterrows():\n",
    "            print(f\"    {row['gene']}: {row['stability_score']:.3f}\")\n",
    "        \n",
    "        stable_genes_df.to_csv('output/Phase_2/approach3_stable_genes.csv', index=False)\n",
    "        print(\"  Full gene list saved to 'output/Phase_2/approach3_stable_genes.csv'\")\n",
    "\n",
    "print(\"\\nPerformance Interpretation:\")\n",
    "mean_auc = np.mean(results_stability['roc_auc'])\n",
    "if mean_auc >= 0.8:\n",
    "    print(\"  EXCELLENT performance (AUC >= 0.8)\")\n",
    "elif mean_auc >= 0.7:\n",
    "    print(\"  GOOD performance (0.7 <= AUC < 0.8)\")\n",
    "elif mean_auc >= 0.6:\n",
    "    print(\"  FAIR performance (0.6 <= AUC < 0.7)\")\n",
    "else:\n",
    "    print(\"  POOR performance (AUC < 0.6)\")\n",
    "\n",
    "approach3_detailed = pd.DataFrame({\n",
    "    'Fold': range(1, len(results_stability['accuracy']) + 1),\n",
    "    'Accuracy': results_stability['accuracy'],\n",
    "    'ROC_AUC': results_stability['roc_auc'],\n",
    "    'MCC': results_stability['mcc']\n",
    "})\n",
    "approach3_detailed.to_csv('output/Phase_2/approach3_detailed.csv', index=False)\n",
    "\n",
    "approach3_summary = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'ROC-AUC', 'MCC', 'Stable_Genes'],\n",
    "    'Mean': [np.mean(results_stability['accuracy']), np.mean(results_stability['roc_auc']),\n",
    "             np.mean(results_stability['mcc']), len(stable_gene_indices)],\n",
    "    'Std': [np.std(results_stability['accuracy']), np.std(results_stability['roc_auc']),\n",
    "            np.std(results_stability['mcc']), 0],\n",
    "    'Min': [np.min(results_stability['accuracy']), np.min(results_stability['roc_auc']),\n",
    "            np.min(results_stability['mcc']), len(stable_gene_indices)],\n",
    "    'Max': [np.max(results_stability['accuracy']), np.max(results_stability['roc_auc']),\n",
    "            np.max(results_stability['mcc']), len(stable_gene_indices)]\n",
    "})\n",
    "approach3_summary.to_csv('output/Phase_2/approach3_summary.csv', index=False)\n",
    "\n",
    "print(\"\\nResults saved:\")\n",
    "print(\"  output/Phase_2/results_approach3.pkl\")\n",
    "print(\"  output/Phase_2/approach3_detailed.csv\")\n",
    "print(\"  output/Phase_2/approach3_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86c2b60a-581f-4f04-8f58-e848d821886b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data from memory\n",
      "\n",
      "APPROACH 4: Mutual Information + RFE Pipeline\n",
      "Dataset: 42 samples × 36157 genes\n",
      "Method: MI pre-filter → RFE fine-tuning\n",
      "\n",
      "APPROACH 4 RESULTS SUMMARY\n",
      "Overall Performance:\n",
      "  Accuracy:     0.528 ± 0.155\n",
      "  ROC-AUC:      0.542 ± 0.140\n",
      "  MCC:          0.062 ± 0.320\n",
      "\n",
      "Per-Fold Breakdown:\n",
      "  Fold 1: Acc=0.333 | AUC=0.350 | MCC=-0.350\n",
      "  Fold 2: Acc=0.556 | AUC=0.550 | MCC=0.158\n",
      "  Fold 3: Acc=0.625 | AUC=0.625 | MCC=0.258\n",
      "  Fold 4: Acc=0.375 | AUC=0.438 | MCC=-0.258\n",
      "  Fold 5: Acc=0.750 | AUC=0.750 | MCC=0.500\n",
      "\n",
      "Best Configuration:\n",
      "  MI features:  500\n",
      "  RFE features: 30\n",
      "  Reduction:    500 → 30 (6.0%)\n",
      "\n",
      "Performance Interpretation:\n",
      "  POOR performance (AUC < 0.6)\n",
      "\n",
      "Results saved:\n",
      "  output/Phase_2/results_approach4.pkl\n",
      "  output/Phase_2/approach4_detailed.csv\n",
      "  output/Phase_2/approach4_summary.csv\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    X_processed\n",
    "    print(\"Using data from memory\")\n",
    "except NameError:\n",
    "    print(\"Loading from disk...\")\n",
    "    with open('output/Phase_2/processed_data.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        X_processed = data['X_processed']\n",
    "        y = data['y']\n",
    "        gene_names_filtered = data['gene_names_filtered']\n",
    "\n",
    "print(\"\\nAPPROACH 4: Mutual Information + RFE Pipeline\")\n",
    "print(f\"Dataset: {X_processed.shape[0]} samples × {X_processed.shape[1]} genes\")\n",
    "print(f\"Method: MI pre-filter → RFE fine-tuning\")\n",
    "\n",
    "# Check if results already exist, if so load them, otherwise train\n",
    "results_file = 'output/Phase_2/results_approach4.pkl'\n",
    "if os.path.exists(results_file):\n",
    "    with open(results_file, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        results_mi_rfe = data['results']\n",
    "        best_config = data['best_config']\n",
    "else:\n",
    "    results_mi_rfe, best_config = mutual_information_rfe_pipeline(X_processed, y)\n",
    "    with open(results_file, 'wb') as f:\n",
    "        pickle.dump({'results': results_mi_rfe, 'best_config': best_config}, f)\n",
    "\n",
    "print(\"\\nAPPROACH 4 RESULTS SUMMARY\")\n",
    "print(\"Overall Performance:\")\n",
    "print(f\"  Accuracy:     {np.mean(results_mi_rfe['accuracy']):.3f} ± {np.std(results_mi_rfe['accuracy']):.3f}\")\n",
    "print(f\"  ROC-AUC:      {np.mean(results_mi_rfe['roc_auc']):.3f} ± {np.std(results_mi_rfe['roc_auc']):.3f}\")\n",
    "print(f\"  MCC:          {np.mean(results_mi_rfe['mcc']):.3f} ± {np.std(results_mi_rfe['mcc']):.3f}\")\n",
    "\n",
    "print(\"\\nPer-Fold Breakdown:\")\n",
    "for fold_idx in range(len(results_mi_rfe['accuracy'])):\n",
    "    print(f\"  Fold {fold_idx+1}: Acc={results_mi_rfe['accuracy'][fold_idx]:.3f} | \"\n",
    "          f\"AUC={results_mi_rfe['roc_auc'][fold_idx]:.3f} | \"\n",
    "          f\"MCC={results_mi_rfe['mcc'][fold_idx]:.3f}\")\n",
    "\n",
    "print(\"\\nBest Configuration:\")\n",
    "print(f\"  MI features:  {best_config[0]}\")\n",
    "print(f\"  RFE features: {best_config[1]}\")\n",
    "print(f\"  Reduction:    {best_config[0]} → {best_config[1]} ({best_config[1]/best_config[0]:.1%})\")\n",
    "\n",
    "print(\"\\nPerformance Interpretation:\")\n",
    "mean_auc = np.mean(results_mi_rfe['roc_auc'])\n",
    "if mean_auc >= 0.8:\n",
    "    print(\"  EXCELLENT performance (AUC >= 0.8)\")\n",
    "elif mean_auc >= 0.7:\n",
    "    print(\"  GOOD performance (0.7 <= AUC < 0.8)\")\n",
    "elif mean_auc >= 0.6:\n",
    "    print(\"  FAIR performance (0.6 <= AUC < 0.7)\")\n",
    "else:\n",
    "    print(\"  POOR performance (AUC < 0.6)\")\n",
    "\n",
    "approach4_detailed = pd.DataFrame({\n",
    "    'Fold': range(1, len(results_mi_rfe['accuracy']) + 1),\n",
    "    'Accuracy': results_mi_rfe['accuracy'],\n",
    "    'ROC_AUC': results_mi_rfe['roc_auc'],\n",
    "    'MCC': results_mi_rfe['mcc']\n",
    "})\n",
    "approach4_detailed.to_csv('output/Phase_2/approach4_detailed.csv', index=False)\n",
    "\n",
    "approach4_summary = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'ROC-AUC', 'MCC', 'MI_Features', 'RFE_Features'],\n",
    "    'Mean': [np.mean(results_mi_rfe['accuracy']), np.mean(results_mi_rfe['roc_auc']),\n",
    "             np.mean(results_mi_rfe['mcc']), best_config[0], best_config[1]],\n",
    "    'Std': [np.std(results_mi_rfe['accuracy']), np.std(results_mi_rfe['roc_auc']),\n",
    "            np.std(results_mi_rfe['mcc']), 0, 0],\n",
    "    'Min': [np.min(results_mi_rfe['accuracy']), np.min(results_mi_rfe['roc_auc']),\n",
    "            np.min(results_mi_rfe['mcc']), best_config[0], best_config[1]],\n",
    "    'Max': [np.max(results_mi_rfe['accuracy']), np.max(results_mi_rfe['roc_auc']),\n",
    "            np.max(results_mi_rfe['mcc']), best_config[0], best_config[1]]\n",
    "})\n",
    "approach4_summary.to_csv('output/Phase_2/approach4_summary.csv', index=False)\n",
    "\n",
    "print(\"\\nResults saved:\")\n",
    "print(\"  output/Phase_2/results_approach4.pkl\")\n",
    "print(\"  output/Phase_2/approach4_detailed.csv\")\n",
    "print(\"  output/Phase_2/approach4_summary.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9df8140-9b66-4f79-be33-615e5b58e52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data from memory\n",
      "\n",
      "APPROACH 5: Feature Bagging Ensemble\n",
      "Dataset: 42 samples × 36157 genes\n",
      "Method: 30 SVMs on random 10% feature subsets\n",
      "\n",
      "APPROACH 5 RESULTS SUMMARY\n",
      "Overall Performance:\n",
      "  Accuracy:     0.453 ± 0.046\n",
      "  ROC-AUC:      0.532 ± 0.188\n",
      "  MCC:          -0.107 ± 0.149\n",
      "  Ensemble:     30 models × 3615 features\n",
      "\n",
      "Per-Fold Breakdown:\n",
      "  Fold 1: Acc=0.444 | AUC=0.450 | MCC=-0.158\n",
      "  Fold 2: Acc=0.444 | AUC=0.900 | MCC=0.000\n",
      "  Fold 3: Acc=0.500 | AUC=0.500 | MCC=0.000\n",
      "  Fold 4: Acc=0.500 | AUC=0.438 | MCC=0.000\n",
      "  Fold 5: Acc=0.375 | AUC=0.375 | MCC=-0.378\n",
      "\n",
      "Ensemble Configuration:\n",
      "  Number of models:     30\n",
      "  Features per model:   3615\n",
      "  Total features used:  36157\n",
      "  Feature fraction:     10%\n",
      "  Aggregation method:   Mean probability\n",
      "\n",
      "Performance Interpretation:\n",
      "  POOR performance (AUC < 0.6)\n",
      "\n",
      "Results saved:\n",
      "  output/Phase_2/results_approach5.pkl\n",
      "  output/Phase_2/approach5_detailed.csv\n",
      "  output/Phase_2/approach5_summary.csv\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    X_processed\n",
    "    print(\"Using data from memory\")\n",
    "except NameError:\n",
    "    print(\"Loading from disk...\")\n",
    "    with open('output/Phase_2/processed_data.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        X_processed = data['X_processed']\n",
    "        y = data['y']\n",
    "        gene_names_filtered = data['gene_names_filtered']\n",
    "\n",
    "print(\"\\nAPPROACH 5: Feature Bagging Ensemble\")\n",
    "print(f\"Dataset: {X_processed.shape[0]} samples × {X_processed.shape[1]} genes\")\n",
    "print(f\"Method: 30 SVMs on random 10% feature subsets\")\n",
    "\n",
    "# Check if results already exist, if so load them, otherwise train\n",
    "results_file = 'output/Phase_2/results_approach5.pkl'\n",
    "if os.path.exists(results_file):\n",
    "    with open(results_file, 'rb') as f:\n",
    "        results_ensemble = pickle.load(f)\n",
    "else:\n",
    "    results_ensemble = ensemble_feature_bagging(X_processed, y, n_estimators=30, feature_fraction=0.1)\n",
    "    with open(results_file, 'wb') as f:\n",
    "        pickle.dump(results_ensemble, f)\n",
    "\n",
    "print(\"\\nAPPROACH 5 RESULTS SUMMARY\")\n",
    "features_per_model = int(X_processed.shape[1] * 0.1)\n",
    "print(\"Overall Performance:\")\n",
    "print(f\"  Accuracy:     {np.mean(results_ensemble['accuracy']):.3f} ± {np.std(results_ensemble['accuracy']):.3f}\")\n",
    "print(f\"  ROC-AUC:      {np.mean(results_ensemble['roc_auc']):.3f} ± {np.std(results_ensemble['roc_auc']):.3f}\")\n",
    "print(f\"  MCC:          {np.mean(results_ensemble['mcc']):.3f} ± {np.std(results_ensemble['mcc']):.3f}\")\n",
    "print(f\"  Ensemble:     30 models × {features_per_model} features\")\n",
    "\n",
    "print(\"\\nPer-Fold Breakdown:\")\n",
    "for fold_idx in range(len(results_ensemble['accuracy'])):\n",
    "    print(f\"  Fold {fold_idx+1}: Acc={results_ensemble['accuracy'][fold_idx]:.3f} | \"\n",
    "          f\"AUC={results_ensemble['roc_auc'][fold_idx]:.3f} | \"\n",
    "          f\"MCC={results_ensemble['mcc'][fold_idx]:.3f}\")\n",
    "\n",
    "print(\"\\nEnsemble Configuration:\")\n",
    "print(f\"  Number of models:     30\")\n",
    "print(f\"  Features per model:   {features_per_model}\")\n",
    "print(f\"  Total features used:  {X_processed.shape[1]}\")\n",
    "print(f\"  Feature fraction:     10%\")\n",
    "print(f\"  Aggregation method:   Mean probability\")\n",
    "\n",
    "print(\"\\nPerformance Interpretation:\")\n",
    "mean_auc = np.mean(results_ensemble['roc_auc'])\n",
    "if mean_auc >= 0.8:\n",
    "    print(\"  EXCELLENT performance (AUC >= 0.8)\")\n",
    "elif mean_auc >= 0.7:\n",
    "    print(\"  GOOD performance (0.7 <= AUC < 0.8)\")\n",
    "elif mean_auc >= 0.6:\n",
    "    print(\"  FAIR performance (0.6 <= AUC < 0.7)\")\n",
    "else:\n",
    "    print(\"  POOR performance (AUC < 0.6)\")\n",
    "\n",
    "approach5_detailed = pd.DataFrame({\n",
    "    'Fold': range(1, len(results_ensemble['accuracy']) + 1),\n",
    "    'Accuracy': results_ensemble['accuracy'],\n",
    "    'ROC_AUC': results_ensemble['roc_auc'],\n",
    "    'MCC': results_ensemble['mcc']\n",
    "})\n",
    "approach5_detailed.to_csv('output/Phase_2/approach5_detailed.csv', index=False)\n",
    "\n",
    "approach5_summary = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'ROC-AUC', 'MCC', 'Ensemble_Size', 'Features_Per_Model'],\n",
    "    'Mean': [np.mean(results_ensemble['accuracy']), np.mean(results_ensemble['roc_auc']),\n",
    "             np.mean(results_ensemble['mcc']), 30, features_per_model],\n",
    "    'Std': [np.std(results_ensemble['accuracy']), np.std(results_ensemble['roc_auc']),\n",
    "            np.std(results_ensemble['mcc']), 0, 0],\n",
    "    'Min': [np.min(results_ensemble['accuracy']), np.min(results_ensemble['roc_auc']),\n",
    "            np.min(results_ensemble['mcc']), 30, features_per_model],\n",
    "    'Max': [np.max(results_ensemble['accuracy']), np.max(results_ensemble['roc_auc']),\n",
    "            np.max(results_ensemble['mcc']), 30, features_per_model]\n",
    "})\n",
    "approach5_summary.to_csv('output/Phase_2/approach5_summary.csv', index=False)\n",
    "\n",
    "print(\"\\nResults saved:\")\n",
    "print(\"  output/Phase_2/results_approach5.pkl\")\n",
    "print(\"  output/Phase_2/approach5_detailed.csv\")\n",
    "print(\"  output/Phase_2/approach5_summary.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "788543b3-62f5-4804-acd3-bb7dcbba77d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data from memory\n",
      "\n",
      "APPROACH 6: OPTIMAL Pipeline (LOO-CV)\n",
      "Dataset: 42 samples × 36157 genes\n",
      "Method: Leave-One-Out CV with nested stability selection\n",
      "WARNING: This will take longer but provides most accurate estimates\n",
      "\n",
      "OPTIMAL PIPELINE RESULTS SUMMARY\n",
      "Overall Performance:\n",
      "  Accuracy:     0.476\n",
      "  ROC-AUC:      0.492\n",
      "  MCC:          -0.048\n",
      "  Sensitivity:  0.476 (Recall, TPR)\n",
      "  Specificity:  0.476 (TNR)\n",
      "  Precision:    0.476 (PPV)\n",
      "  F1-Score:     0.476\n",
      "  NPV:          0.476\n",
      "\n",
      "Confusion Matrix:\n",
      "                 Predicted Control  Predicted MDD\n",
      "  True Control           10               11\n",
      "  True MDD               11               10\n",
      "\n",
      "  True Negatives  (TN): 10\n",
      "  False Positives (FP): 11\n",
      "  False Negatives (FN): 11\n",
      "  True Positives  (TP): 10\n",
      "\n",
      "Sample Predictions (First 10):\n",
      "  Sample  1: True=1 | Pred=1 | Prob=0.635 CORRECT\n",
      "  Sample  2: True=1 | Pred=0 | Prob=0.199 WRONG\n",
      "  Sample  3: True=1 | Pred=1 | Prob=0.926 CORRECT\n",
      "  Sample  4: True=1 | Pred=1 | Prob=0.783 CORRECT\n",
      "  Sample  5: True=1 | Pred=1 | Prob=0.957 CORRECT\n",
      "  Sample  6: True=1 | Pred=1 | Prob=0.973 CORRECT\n",
      "  Sample  7: True=1 | Pred=0 | Prob=0.314 WRONG\n",
      "  Sample  8: True=1 | Pred=0 | Prob=0.394 WRONG\n",
      "  Sample  9: True=1 | Pred=0 | Prob=0.264 WRONG\n",
      "  Sample 10: True=1 | Pred=0 | Prob=0.350 WRONG\n",
      "\n",
      "Gene Selection Consistency:\n",
      "  Genes selected >50% of folds: 13\n",
      "  Genes selected >75% of folds: 6\n",
      "  Genes selected 100% of folds:  0\n",
      "\n",
      "  Top 10 Most Consistently Selected Genes:\n",
      "    PPAP2C: 95.2%\n",
      "    LOC648517: 90.5%\n",
      "    LOC284948: 88.1%\n",
      "    ASB14: 88.1%\n",
      "    CLDN4: 78.6%\n",
      "    SFTPD: 76.2%\n",
      "    YARS: 73.8%\n",
      "    USP9X: 73.8%\n",
      "    POU3F3: 66.7%\n",
      "    DPP7: 64.3%\n",
      "\n",
      "Performance Interpretation:\n",
      "  POOR performance (AUC < 0.6)\n",
      "  No better than random guessing\n",
      "\n",
      "Results saved:\n",
      "  output/Phase_2/results_optimal.pkl\n",
      "  output/Phase_2/optimal_predictions.csv\n",
      "  output/Phase_2/optimal_confusion_matrix.csv\n",
      "  output/Phase_2/optimal_gene_consistency.csv\n",
      "  output/Phase_2/optimal_summary.csv\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    X_processed\n",
    "    print(\"Using data from memory\")\n",
    "except NameError:\n",
    "    print(\"Loading from disk...\")\n",
    "    with open('output/Phase_2/processed_data.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        X_processed = data['X_processed']\n",
    "        y = data['y']\n",
    "        gene_names_filtered = data['gene_names_filtered']\n",
    "\n",
    "print(\"\\nAPPROACH 6: OPTIMAL Pipeline (LOO-CV)\")\n",
    "print(f\"Dataset: {X_processed.shape[0]} samples × {X_processed.shape[1]} genes\")\n",
    "print(f\"Method: Leave-One-Out CV with nested stability selection\")\n",
    "print(\"WARNING: This will take longer but provides most accurate estimates\")\n",
    "\n",
    "# Check if results already exist, if so load them, otherwise train\n",
    "results_file = 'output/Phase_2/results_optimal.pkl'\n",
    "if os.path.exists(results_file):\n",
    "    with open(results_file, 'rb') as f:\n",
    "        results_optimal = pickle.load(f)\n",
    "else:\n",
    "    results_optimal = optimal_nested_cv_pipeline(X_processed, y, n_genes=50)\n",
    "    with open(results_file, 'wb') as f:\n",
    "        pickle.dump(results_optimal, f)\n",
    "\n",
    "print(\"\\nOPTIMAL PIPELINE RESULTS SUMMARY\")\n",
    "metrics = results_optimal['metrics']\n",
    "cm = metrics['confusion_matrix']\n",
    "\n",
    "print(\"Overall Performance:\")\n",
    "print(f\"  Accuracy:     {metrics['accuracy']:.3f}\")\n",
    "print(f\"  ROC-AUC:      {metrics['auc']:.3f}\")\n",
    "print(f\"  MCC:          {metrics['mcc']:.3f}\")\n",
    "\n",
    "tn, fp, fn, tp = cm[0,0], cm[0,1], cm[1,0], cm[1,1]\n",
    "sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "f1_score = 2 * (precision * sensitivity) / (precision + sensitivity) if (precision + sensitivity) > 0 else 0\n",
    "npv = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "\n",
    "print(f\"  Sensitivity:  {sensitivity:.3f} (Recall, TPR)\")\n",
    "print(f\"  Specificity:  {specificity:.3f} (TNR)\")\n",
    "print(f\"  Precision:    {precision:.3f} (PPV)\")\n",
    "print(f\"  F1-Score:     {f1_score:.3f}\")\n",
    "print(f\"  NPV:          {npv:.3f}\")\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(f\"                 Predicted Control  Predicted MDD\")\n",
    "print(f\"  True Control          {tn:3d}              {fp:3d}\")\n",
    "print(f\"  True MDD              {fn:3d}              {tp:3d}\")\n",
    "print(f\"\\n  True Negatives  (TN): {tn}\")\n",
    "print(f\"  False Positives (FP): {fp}\")\n",
    "print(f\"  False Negatives (FN): {fn}\")\n",
    "print(f\"  True Positives  (TP): {tp}\")\n",
    "\n",
    "print(\"\\nSample Predictions (First 10):\")\n",
    "predictions = np.array(results_optimal['predictions'])\n",
    "probabilities = np.array(results_optimal['probabilities'])\n",
    "true_labels = np.array(results_optimal['true_labels'])\n",
    "\n",
    "for i in range(min(10, len(predictions))):\n",
    "    status = \"CORRECT\" if predictions[i] == true_labels[i] else \"WRONG\"\n",
    "    print(f\"  Sample {i+1:2d}: True={true_labels[i]} | Pred={predictions[i]} | Prob={probabilities[i]:.3f} {status}\")\n",
    "\n",
    "print(\"\\nGene Selection Consistency:\")\n",
    "gene_selection_freq = metrics['gene_selection_frequency']\n",
    "consistent_genes = metrics['consistent_genes']\n",
    "\n",
    "print(f\"  Genes selected >50% of folds: {len(consistent_genes)}\")\n",
    "print(f\"  Genes selected >75% of folds: {np.sum(gene_selection_freq > 0.75)}\")\n",
    "print(f\"  Genes selected 100% of folds:  {np.sum(gene_selection_freq == 1.0)}\")\n",
    "\n",
    "if len(consistent_genes) > 0 and gene_names_filtered:\n",
    "    top_consistent_indices = np.argsort(gene_selection_freq)[-20:][::-1]\n",
    "    valid_indices = [i for i in top_consistent_indices if i < len(gene_names_filtered)]\n",
    "    \n",
    "    if valid_indices:\n",
    "        print(\"\\n  Top 10 Most Consistently Selected Genes:\")\n",
    "        for idx in valid_indices[:10]:\n",
    "            print(f\"    {gene_names_filtered[idx]}: {gene_selection_freq[idx]:.1%}\")\n",
    "\n",
    "print(\"\\nPerformance Interpretation:\")\n",
    "if metrics['auc'] >= 0.8:\n",
    "    print(\"  EXCELLENT performance (AUC >= 0.8)\")\n",
    "elif metrics['auc'] >= 0.7:\n",
    "    print(\"  GOOD performance (0.7 <= AUC < 0.8)\")\n",
    "elif metrics['auc'] >= 0.6:\n",
    "    print(\"  FAIR performance (0.6 <= AUC < 0.7)\")\n",
    "else:\n",
    "    print(\"  POOR performance (AUC < 0.6)\")\n",
    "\n",
    "if metrics['mcc'] >= 0.5:\n",
    "    print(\"  Strong correlation between predictions and reality\")\n",
    "elif metrics['mcc'] >= 0.3:\n",
    "    print(\"  Moderate correlation\")\n",
    "elif metrics['mcc'] >= 0.1:\n",
    "    print(\"  Weak correlation\")\n",
    "else:\n",
    "    print(\"  No better than random guessing\")\n",
    "\n",
    "optimal_predictions = pd.DataFrame({\n",
    "    'Sample': range(1, len(predictions) + 1),\n",
    "    'True_Label': true_labels,\n",
    "    'Predicted_Label': predictions,\n",
    "    'Probability_MDD': probabilities,\n",
    "    'Correct': predictions == true_labels\n",
    "})\n",
    "optimal_predictions.to_csv('output/Phase_2/optimal_predictions.csv', index=False)\n",
    "\n",
    "cm_df = pd.DataFrame(cm, index=['True_Control', 'True_MDD'], columns=['Pred_Control', 'Pred_MDD'])\n",
    "cm_df.to_csv('output/Phase_2/optimal_confusion_matrix.csv')\n",
    "\n",
    "if gene_names_filtered:\n",
    "    valid_mask = np.arange(len(gene_selection_freq)) < len(gene_names_filtered)\n",
    "    gene_consistency = pd.DataFrame({\n",
    "        'gene': [gene_names_filtered[i] for i in range(len(gene_names_filtered)) if valid_mask[i]],\n",
    "        'selection_frequency': gene_selection_freq[valid_mask]\n",
    "    }).sort_values('selection_frequency', ascending=False)\n",
    "    gene_consistency.to_csv('output/Phase_2/optimal_gene_consistency.csv', index=False)\n",
    "\n",
    "optimal_summary = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'ROC-AUC', 'MCC', 'Sensitivity', 'Specificity', \n",
    "               'Precision', 'F1-Score', 'NPV', 'Consistent_Genes'],\n",
    "    'Value': [metrics['accuracy'], metrics['auc'], metrics['mcc'], sensitivity, \n",
    "              specificity, precision, f1_score, npv, len(consistent_genes)]\n",
    "})\n",
    "optimal_summary.to_csv('output/Phase_2/optimal_summary.csv', index=False)\n",
    "\n",
    "print(\"\\nResults saved:\")\n",
    "print(\"  output/Phase_2/results_optimal.pkl\")\n",
    "print(\"  output/Phase_2/optimal_predictions.csv\")\n",
    "print(\"  output/Phase_2/optimal_confusion_matrix.csv\")\n",
    "print(\"  output/Phase_2/optimal_gene_consistency.csv\")\n",
    "print(\"  output/Phase_2/optimal_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17881447-b277-485d-9986-e2bbedb3ed1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data from memory\n",
      "\n",
      "FINAL MODEL TRAINING FOR DEPLOYMENT\n",
      "Dataset: 42 samples × 36157 genes\n",
      "Training on 100% of data for production use\n",
      "\n",
      "Pre-Training Verification:\n",
      "  X_processed shape:  (42, 36157)\n",
      "  y shape:            (42,)\n",
      "  gene_names length:  36157\n",
      "\n",
      "Training final model...\n",
      "Feature selection complete: 50 genes selected\n",
      "Scaling complete\n",
      "Classifier training complete\n",
      "\n",
      "Training Set Performance:\n",
      "  Accuracy:  1.000\n",
      "  ROC-AUC:   1.000\n",
      "  MCC:       1.000\n",
      "  Note: These are optimistic estimates (trained on same data)\n",
      "\n",
      "Selected Gene Analysis:\n",
      "  Number of genes selected: 50\n",
      "  Index range: 240 to 36149\n",
      "  Gene names available: 36157\n",
      "\n",
      "  Top 20 Most Important Genes:\n",
      "     1. LOC284948            (score: 0.232)\n",
      "     2. NTRK1                (score: 0.170)\n",
      "     3. PPAP2C               (score: 0.154)\n",
      "     4. LOC648517            (score: 0.138)\n",
      "     5. WDR63                (score: 0.122)\n",
      "     6. WTAP                 (score: 0.114)\n",
      "     7. LOC653513            (score: 0.110)\n",
      "     8. POGZ                 (score: 0.104)\n",
      "     9. LOC342931            (score: 0.096)\n",
      "    10. PCDH24               (score: 0.096)\n",
      "    11. CD86                 (score: 0.090)\n",
      "    12. CCR2                 (score: 0.088)\n",
      "    13. LOC644899            (score: 0.086)\n",
      "    14. LOC644516            (score: 0.084)\n",
      "    15. TMPRSS3              (score: 0.080)\n",
      "    16. PGAM5                (score: 0.078)\n",
      "    17. TLR8                 (score: 0.078)\n",
      "    18. LOC728635            (score: 0.074)\n",
      "    19. LOC642628            (score: 0.074)\n",
      "    20. LOC643341            (score: 0.074)\n",
      "  Full gene list saved to 'output/Phase_2/final_model_gene_importance.csv'\n",
      "\n",
      "Packaging Final Model:\n",
      "  Model components: Selector, Scaler, Classifier, Gene importance, Metadata\n",
      "\n",
      "Final model saved:\n",
      "  output/Phase_2/final_model.pkl\n",
      "  output/Phase_2/final_model_gene_importance.csv\n",
      "  output/Phase_2/final_model_metadata.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    X_processed\n",
    "    print(\"Using data from memory\")\n",
    "except NameError:\n",
    "    print(\"Loading from disk...\")\n",
    "    with open('output/Phase_2/processed_data.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        X_processed = data['X_processed']\n",
    "        y = data['y']\n",
    "        gene_names_filtered = data['gene_names_filtered']\n",
    "\n",
    "print(\"\\nFINAL MODEL TRAINING FOR DEPLOYMENT\")\n",
    "print(f\"Dataset: {X_processed.shape[0]} samples × {X_processed.shape[1]} genes\")\n",
    "print(f\"Training on 100% of data for production use\")\n",
    "\n",
    "print(\"\\nPre-Training Verification:\")\n",
    "print(f\"  X_processed shape:  {X_processed.shape}\")\n",
    "print(f\"  y shape:            {y.shape}\")\n",
    "print(f\"  gene_names length:  {len(gene_names_filtered) if gene_names_filtered else None}\")\n",
    "\n",
    "if gene_names_filtered and len(gene_names_filtered) != X_processed.shape[1]:\n",
    "    print(f\"  WARNING: Mismatch detected!\")\n",
    "    print(f\"     Features: {X_processed.shape[1]}\")\n",
    "    print(f\"     Gene names: {len(gene_names_filtered)}\")\n",
    "    print(f\"  Truncating gene_names to match features\")\n",
    "    gene_names_filtered = gene_names_filtered[:X_processed.shape[1]]\n",
    "\n",
    "print(\"\\nTraining final model...\")\n",
    "\n",
    "# Check if final model already exists, if so load it, otherwise train\n",
    "model_file = 'output/Phase_2/final_model.pkl'\n",
    "if os.path.exists(model_file):\n",
    "    with open(model_file, 'rb') as f:\n",
    "        final_pipeline = pickle.load(f)\n",
    "    \n",
    "    selector = final_pipeline['selector']\n",
    "    scaler = final_pipeline['scaler']\n",
    "    final_clf = final_pipeline['classifier']\n",
    "    selected_indices = final_pipeline['selected_indices']\n",
    "    train_acc = final_pipeline['training_performance']['accuracy']\n",
    "    train_auc = final_pipeline['training_performance']['roc_auc']\n",
    "    train_mcc = final_pipeline['training_performance']['mcc']\n",
    "    selected_gene_names = final_pipeline.get('selected_genes', None)\n",
    "    gene_importance = final_pipeline.get('gene_importance', None)\n",
    "    X_selected = selector.transform(X_processed)\n",
    "else:\n",
    "    selector = StableGeneSelector(n_genes_to_select=min(50, X_processed.shape[1]),\n",
    "                                 n_bootstrap=50, stability_threshold=0.5, random_state=42)\n",
    "    X_selected = selector.fit_transform(X_processed, y)\n",
    "    print(f\"Feature selection complete: {X_selected.shape[1]} genes selected\")\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_selected)\n",
    "    print(\"Scaling complete\")\n",
    "\n",
    "    final_clf = SVC(kernel='rbf', C=1.0, gamma='scale', probability=True, random_state=42)\n",
    "    final_clf.fit(X_scaled, y)\n",
    "    print(\"Classifier training complete\")\n",
    "\n",
    "    print(\"\\nTraining Set Performance:\")\n",
    "    y_pred_train = final_clf.predict(X_scaled)\n",
    "    y_prob_train = final_clf.predict_proba(X_scaled)[:, 1]\n",
    "\n",
    "    train_acc = accuracy_score(y, y_pred_train)\n",
    "    train_auc = roc_auc_score(y, y_prob_train)\n",
    "    train_mcc = matthews_corrcoef(y, y_pred_train)\n",
    "    \n",
    "    selected_indices = selector.selected_genes_\n",
    "    \n",
    "    if gene_names_filtered:\n",
    "        max_idx = max(selected_indices)\n",
    "        if max_idx >= len(gene_names_filtered):\n",
    "            selected_gene_names = None\n",
    "            gene_importance = None\n",
    "        else:\n",
    "            selected_gene_names = [gene_names_filtered[i] for i in selected_indices]\n",
    "            gene_importance = pd.DataFrame({\n",
    "                'gene': selected_gene_names,\n",
    "                'stability_score': selector.gene_scores_[selected_indices]\n",
    "            }).sort_values('stability_score', ascending=False)\n",
    "            gene_importance['rank'] = range(1, len(gene_importance) + 1)\n",
    "            gene_importance = gene_importance[['rank', 'gene', 'stability_score']]\n",
    "    else:\n",
    "        selected_gene_names = None\n",
    "        gene_importance = None\n",
    "    \n",
    "    final_pipeline = {\n",
    "        'selector': selector,\n",
    "        'scaler': scaler,\n",
    "        'classifier': final_clf,\n",
    "        'selected_indices': selected_indices,\n",
    "        'training_performance': {'accuracy': train_acc, 'roc_auc': train_auc, 'mcc': train_mcc},\n",
    "        'metadata': {\n",
    "            'n_training_samples': X_processed.shape[0],\n",
    "            'n_original_features': X_processed.shape[1],\n",
    "            'n_selected_features': X_selected.shape[1],\n",
    "            'feature_selection_method': 'StableGeneSelector',\n",
    "            'n_bootstrap': 50,\n",
    "            'stability_threshold': 0.5,\n",
    "            'classifier': 'RBF-SVM',\n",
    "            'hyperparameters': {'C': 1.0, 'gamma': 'scale', 'kernel': 'rbf'}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if selected_gene_names:\n",
    "        final_pipeline['selected_genes'] = selected_gene_names\n",
    "        final_pipeline['gene_importance'] = gene_importance\n",
    "    \n",
    "    with open(model_file, 'wb') as f:\n",
    "        pickle.dump(final_pipeline, f)\n",
    "\n",
    "print(f\"Feature selection complete: {X_selected.shape[1]} genes selected\")\n",
    "print(\"Scaling complete\")\n",
    "print(\"Classifier training complete\")\n",
    "\n",
    "print(\"\\nTraining Set Performance:\")\n",
    "print(f\"  Accuracy:  {train_acc:.3f}\")\n",
    "print(f\"  ROC-AUC:   {train_auc:.3f}\")\n",
    "print(f\"  MCC:       {train_mcc:.3f}\")\n",
    "print(\"  Note: These are optimistic estimates (trained on same data)\")\n",
    "\n",
    "print(\"\\nSelected Gene Analysis:\")\n",
    "\n",
    "print(f\"  Number of genes selected: {len(selected_indices)}\")\n",
    "print(f\"  Index range: {min(selected_indices)} to {max(selected_indices)}\")\n",
    "\n",
    "if gene_names_filtered:\n",
    "    print(f\"  Gene names available: {len(gene_names_filtered)}\")\n",
    "    \n",
    "    max_idx = max(selected_indices)\n",
    "    if max_idx >= len(gene_names_filtered):\n",
    "        print(f\"  ERROR: Max index {max_idx} exceeds gene_names length {len(gene_names_filtered)}\")\n",
    "        print(f\"  Cannot create gene importance table\")\n",
    "    else:\n",
    "        print(f\"\\n  Top 20 Most Important Genes:\")\n",
    "        for _, row in gene_importance.head(20).iterrows():\n",
    "            print(f\"    {row['rank']:2d}. {row['gene']:20s} (score: {row['stability_score']:.3f})\")\n",
    "        \n",
    "        gene_importance.to_csv('output/Phase_2/final_model_gene_importance.csv', index=False)\n",
    "        print(f\"  Full gene list saved to 'output/Phase_2/final_model_gene_importance.csv'\")\n",
    "else:\n",
    "    print(\"  No gene names available - saving indices only\")\n",
    "\n",
    "print(\"\\nPackaging Final Model:\")\n",
    "print(\"  Model components: Selector, Scaler, Classifier, Gene importance, Metadata\")\n",
    "\n",
    "metadata_df = pd.DataFrame({\n",
    "    'Parameter': ['Training Samples', 'Original Features', 'Selected Features', \n",
    "                  'Feature Selection Method', 'Bootstrap Iterations', 'Stability Threshold',\n",
    "                  'Classifier Type', 'SVM Kernel', 'SVM C', 'SVM Gamma',\n",
    "                  'Training Accuracy', 'Training ROC-AUC', 'Training MCC'],\n",
    "    'Value': [final_pipeline['metadata']['n_training_samples'],\n",
    "              final_pipeline['metadata']['n_original_features'],\n",
    "              final_pipeline['metadata']['n_selected_features'],\n",
    "              final_pipeline['metadata']['feature_selection_method'],\n",
    "              final_pipeline['metadata']['n_bootstrap'],\n",
    "              final_pipeline['metadata']['stability_threshold'],\n",
    "              final_pipeline['metadata']['classifier'],\n",
    "              final_pipeline['metadata']['hyperparameters']['kernel'],\n",
    "              final_pipeline['metadata']['hyperparameters']['C'],\n",
    "              final_pipeline['metadata']['hyperparameters']['gamma'],\n",
    "              f\"{train_acc:.3f}\", f\"{train_auc:.3f}\", f\"{train_mcc:.3f}\"]\n",
    "})\n",
    "metadata_df.to_csv('output/Phase_2/final_model_metadata.csv', index=False)\n",
    "\n",
    "print(\"\\nFinal model saved:\")\n",
    "print(\"  output/Phase_2/final_model.pkl\")\n",
    "print(\"  output/Phase_2/final_model_gene_importance.csv\")\n",
    "print(\"  output/Phase_2/final_model_metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b0c3f54-0c51-44d5-a760-9542e16eaa24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "COMPREHENSIVE RESULTS SUMMARY\n",
      "\n",
      "Checking saved results...\n",
      "  Approach 1 results loaded\n",
      "  Approach 2 results loaded\n",
      "  Approach 3 results loaded\n",
      "  Approach 4 results loaded\n",
      "  Approach 5 results loaded\n",
      "  Optimal pipeline results loaded\n",
      "  Final model loaded\n",
      "\n",
      "PERFORMANCE COMPARISON:\n",
      "        Approach      Accuracy       ROC-AUC            MCC  Rank\n",
      "      Approach 1 0.525 ± 0.061 0.685 ± 0.074  0.072 ± 0.101     1\n",
      "      Approach 2 0.578 ± 0.118 0.555 ± 0.166  0.243 ± 0.223     2\n",
      "      Approach 4 0.528 ± 0.155 0.542 ± 0.140  0.062 ± 0.320     3\n",
      "      Approach 5 0.453 ± 0.046 0.532 ± 0.188 -0.107 ± 0.149     4\n",
      "Optimal (LOO-CV)         0.476         0.492         -0.048     5\n",
      "      Approach 3 0.778 ± 0.272 0.200 ± 0.245  0.600 ± 0.490     6\n",
      "\n",
      "BEST PERFORMING APPROACH:\n",
      "  Approach 1\n",
      "  ROC-AUC: 0.685 ± 0.074\n",
      "\n",
      "DETAILED METRICS BREAKDOWN:\n",
      "\n",
      "Accuracy:\n",
      "  Best:  Approach 3           = 0.778\n",
      "  Worst: Approach 5           = 0.453\n",
      "  Range: 0.325\n",
      "\n",
      "ROC-AUC:\n",
      "  Best:  Approach 1           = 0.685\n",
      "  Worst: Approach 3           = 0.200\n",
      "  Range: 0.485\n",
      "\n",
      "MCC:\n",
      "  Best:  Approach 3           = 0.600\n",
      "  Worst: Approach 5           = -0.107\n",
      "  Range: 0.707\n",
      "\n",
      "SAVED FILES INVENTORY:\n",
      "  all_approaches_comparison.csv                 0.4 KB\n",
      "  approach1_detailed.csv                        0.3 KB\n",
      "  approach1_summary.csv                         0.3 KB\n",
      "  approach2_detailed.csv                        0.2 KB\n",
      "  approach2_summary.csv                         0.3 KB\n",
      "  approach3_detailed.csv                        0.1 KB\n",
      "  approach3_stable_genes.csv                    0.8 KB\n",
      "  approach3_summary.csv                         0.2 KB\n",
      "  approach4_detailed.csv                        0.2 KB\n",
      "  approach4_summary.csv                         0.3 KB\n",
      "  approach5_detailed.csv                        0.2 KB\n",
      "  approach5_summary.csv                         0.3 KB\n",
      "  final_model.pkl                             304.6 KB\n",
      "  final_model_gene_importance.csv               1.0 KB\n",
      "  final_model_metadata.csv                      0.3 KB\n",
      "  optimal_confusion_matrix.csv                  0.1 KB\n",
      "  optimal_gene_consistency.csv                438.3 KB\n",
      "  optimal_predictions.csv                       1.4 KB\n",
      "  optimal_summary.csv                           0.3 KB\n",
      "  processed_data.pkl                        12149.2 KB\n",
      "  raw_data.pkl                              13150.8 KB\n",
      "  results_approach1.pkl                         0.4 KB\n",
      "  results_approach2.pkl                         0.4 KB\n",
      "  results_approach3.pkl                       283.3 KB\n",
      "  results_approach4.pkl                         0.2 KB\n",
      "  results_approach5.pkl                         0.2 KB\n",
      "  results_optimal.pkl                         303.6 KB\n",
      "\n",
      "  Total: 27 files, 26637.4 KB\n",
      "\n",
      "RECOMMENDATIONS:\n",
      "  Fair performance\n",
      "  Recommended: Further feature engineering or try different algorithms\n",
      "  Consider: Investigating biological heterogeneity in the cohort\n",
      "\n",
      "Comprehensive summary complete\n",
      "All results saved to 'output/Phase_2/all_approaches_comparison.csv'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "print(\"\\nCOMPREHENSIVE RESULTS SUMMARY\")\n",
    "\n",
    "print(\"\\nChecking saved results...\")\n",
    "\n",
    "available_results = {}\n",
    "\n",
    "for i in range(1, 6):\n",
    "    pkl_path = f'output/Phase_2/results_approach{i}.pkl'\n",
    "    if os.path.exists(pkl_path):\n",
    "        with open(pkl_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            if isinstance(data, dict) and 'results' in data:\n",
    "                available_results[f'Approach {i}'] = data['results']\n",
    "            else:\n",
    "                available_results[f'Approach {i}'] = data\n",
    "        print(f\"  Approach {i} results loaded\")\n",
    "    else:\n",
    "        print(f\"  Approach {i} not found\")\n",
    "\n",
    "if os.path.exists('output/Phase_2/results_optimal.pkl'):\n",
    "    with open('output/Phase_2/results_optimal.pkl', 'rb') as f:\n",
    "        optimal_data = pickle.load(f)\n",
    "        available_results['Optimal (LOO-CV)'] = optimal_data\n",
    "    print(f\"  Optimal pipeline results loaded\")\n",
    "\n",
    "if os.path.exists('output/Phase_2/final_model.pkl'):\n",
    "    with open('output/Phase_2/final_model.pkl', 'rb') as f:\n",
    "        final_model = pickle.load(f)\n",
    "    print(f\"  Final model loaded\")\n",
    "\n",
    "print(\"\\nPERFORMANCE COMPARISON:\")\n",
    "\n",
    "comparison_data = []\n",
    "\n",
    "for name, results in available_results.items():\n",
    "    if name == 'Optimal (LOO-CV)':\n",
    "        metrics = results['metrics']\n",
    "        comparison_data.append({\n",
    "            'Approach': name,\n",
    "            'Accuracy': f\"{metrics['accuracy']:.3f}\",\n",
    "            'ROC-AUC': f\"{metrics['auc']:.3f}\",\n",
    "            'MCC': f\"{metrics['mcc']:.3f}\",\n",
    "            'Rank': 0\n",
    "        })\n",
    "    else:\n",
    "        comparison_data.append({\n",
    "            'Approach': name,\n",
    "            'Accuracy': f\"{np.mean(results['accuracy']):.3f} ± {np.std(results['accuracy']):.3f}\",\n",
    "            'ROC-AUC': f\"{np.mean(results['roc_auc']):.3f} ± {np.std(results['roc_auc']):.3f}\",\n",
    "            'MCC': f\"{np.mean(results['mcc']):.3f} ± {np.std(results['mcc']):.3f}\",\n",
    "            'Rank': 0\n",
    "        })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "auc_values = []\n",
    "for auc_str in comparison_df['ROC-AUC']:\n",
    "    auc_val = float(auc_str.split('±')[0].strip())\n",
    "    auc_values.append(auc_val)\n",
    "\n",
    "comparison_df['Rank'] = pd.Series(auc_values).rank(ascending=False, method='min').astype(int)\n",
    "comparison_df = comparison_df.sort_values('Rank')\n",
    "\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "comparison_df.to_csv('output/Phase_2/all_approaches_comparison.csv', index=False)\n",
    "\n",
    "print(\"\\nBEST PERFORMING APPROACH:\")\n",
    "\n",
    "best_approach = comparison_df.iloc[0]['Approach']\n",
    "best_auc = comparison_df.iloc[0]['ROC-AUC']\n",
    "\n",
    "print(f\"  {best_approach}\")\n",
    "print(f\"  ROC-AUC: {best_auc}\")\n",
    "\n",
    "print(\"\\nDETAILED METRICS BREAKDOWN:\")\n",
    "\n",
    "all_metrics = {'Accuracy': [], 'ROC-AUC': [], 'MCC': []}\n",
    "approach_names = []\n",
    "\n",
    "for name, results in available_results.items():\n",
    "    approach_names.append(name)\n",
    "    \n",
    "    if name == 'Optimal (LOO-CV)':\n",
    "        metrics = results['metrics']\n",
    "        all_metrics['Accuracy'].append(metrics['accuracy'])\n",
    "        all_metrics['ROC-AUC'].append(metrics['auc'])\n",
    "        all_metrics['MCC'].append(metrics['mcc'])\n",
    "    else:\n",
    "        all_metrics['Accuracy'].append(np.mean(results['accuracy']))\n",
    "        all_metrics['ROC-AUC'].append(np.mean(results['roc_auc']))\n",
    "        all_metrics['MCC'].append(np.mean(results['mcc']))\n",
    "\n",
    "for metric_name, values in all_metrics.items():\n",
    "    if values:\n",
    "        best_idx = np.argmax(values)\n",
    "        worst_idx = np.argmin(values)\n",
    "        \n",
    "        print(f\"\\n{metric_name}:\")\n",
    "        print(f\"  Best:  {approach_names[best_idx]:20s} = {values[best_idx]:.3f}\")\n",
    "        print(f\"  Worst: {approach_names[worst_idx]:20s} = {values[worst_idx]:.3f}\")\n",
    "        print(f\"  Range: {max(values) - min(values):.3f}\")\n",
    "\n",
    "print(\"\\nSAVED FILES INVENTORY:\")\n",
    "\n",
    "all_files = glob.glob('output/Phase_2/*.pkl') + glob.glob('output/Phase_2/*.csv')\n",
    "all_files.sort()\n",
    "\n",
    "total_size = 0\n",
    "for filepath in all_files:\n",
    "    size_kb = os.path.getsize(filepath) / 1024\n",
    "    total_size += size_kb\n",
    "    print(f\"  {os.path.basename(filepath):40s} {size_kb:8.1f} KB\")\n",
    "\n",
    "print(f\"\\n  Total: {len(all_files)} files, {total_size:.1f} KB\")\n",
    "\n",
    "print(\"\\nRECOMMENDATIONS:\")\n",
    "\n",
    "best_auc_value = max(all_metrics['ROC-AUC'])\n",
    "\n",
    "if best_auc_value >= 0.8:\n",
    "    print(\"  Excellent performance achieved\")\n",
    "    print(\"  Recommended: Deploy the final model\")\n",
    "    print(\"  Consider: External validation on independent dataset\")\n",
    "elif best_auc_value >= 0.7:\n",
    "    print(\"  Good performance achieved\")\n",
    "    print(\"  Recommended: Use for preliminary screening\")\n",
    "    print(\"  Consider: Collecting more samples or additional features\")\n",
    "elif best_auc_value >= 0.6:\n",
    "    print(\"  Fair performance\")\n",
    "    print(\"  Recommended: Further feature engineering or try different algorithms\")\n",
    "    print(\"  Consider: Investigating biological heterogeneity in the cohort\")\n",
    "else:\n",
    "    print(\"  Poor performance\")\n",
    "    print(\"  Recommended: Re-evaluate approach\")\n",
    "    print(\"  Consider: Different feature types, larger sample size, or problem reformulation\")\n",
    "\n",
    "print(\"\\nComprehensive summary complete\")\n",
    "print(\"All results saved to 'output/Phase_2/all_approaches_comparison.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "67777101-4176-4e73-b120-ce2e18add983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TESTING PREDICTION FUNCTION\n",
      "Using final model from memory\n",
      "Using data from memory\n",
      "\n",
      "Making predictions on training data (sanity check):\n",
      "\n",
      "First 10 samples:\n",
      "Sample    True   Pred  Probability  Confidence  Status \n",
      "1         MDD    MDD      0.991       0.491     MATCH  \n",
      "2         MDD    MDD      0.991       0.491     MATCH  \n",
      "3         MDD    MDD      0.991       0.491     MATCH  \n",
      "4         MDD    MDD      0.991       0.491     MATCH  \n",
      "5         MDD    MDD      0.991       0.491     MATCH  \n",
      "6         MDD    MDD      0.991       0.491     MATCH  \n",
      "7         MDD    MDD      0.991       0.491     MATCH  \n",
      "8         MDD    MDD      0.991       0.491     MATCH  \n",
      "9         MDD    MDD      0.991       0.491     MATCH  \n",
      "10        MDD    MDD      0.991       0.491     MATCH  \n",
      "\n",
      "Training set accuracy: 1.000\n",
      "(Should be high since model was trained on this data)\n",
      "\n",
      "Prediction Confidence Distribution:\n",
      "  High confidence (>0.4):       42 samples (100.0%)\n",
      "  Medium confidence (0.2-0.4):   0 samples (0.0%)\n",
      "  Low confidence (<0.2):         0 samples (0.0%)\n",
      "\n",
      "Saving standalone prediction function...\n",
      "Standalone prediction function saved to 'output/Phase_2/predict_mdd.py'\n"
     ]
    }
   ],
   "source": [
    "def predict_new_samples(pipeline, X_new):\n",
    "    \"\"\"\n",
    "    Make predictions on new samples using the trained pipeline.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    pipeline : dict\n",
    "        The final pipeline from train_final_model()\n",
    "    X_new : array-like, shape (n_samples, n_genes)\n",
    "        New samples to predict. Must be preprocessed the same way as training data.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    predictions : array\n",
    "        Binary predictions (0=Control, 1=MDD)\n",
    "    probabilities : array\n",
    "        Probability of MDD class (0 to 1)\n",
    "    confidence : array\n",
    "        Prediction confidence (distance from 0.5 threshold)\n",
    "    \"\"\"\n",
    "    \n",
    "    X_selected = pipeline['selector'].transform(X_new)\n",
    "    X_scaled = pipeline['scaler'].transform(X_selected)\n",
    "    predictions = pipeline['classifier'].predict(X_scaled)\n",
    "    probabilities = pipeline['classifier'].predict_proba(X_scaled)[:, 1]\n",
    "    confidence = np.abs(probabilities - 0.5)\n",
    "    \n",
    "    return predictions, probabilities, confidence\n",
    "\n",
    "print(\"\\nTESTING PREDICTION FUNCTION\")\n",
    "\n",
    "try:\n",
    "    final_pipeline\n",
    "    print(\"Using final model from memory\")\n",
    "except NameError:\n",
    "    print(\"Loading final model from disk...\")\n",
    "    with open('output/Phase_2/final_model.pkl', 'rb') as f:\n",
    "        final_pipeline = pickle.load(f)\n",
    "\n",
    "try:\n",
    "    X_processed\n",
    "    print(\"Using data from memory\")\n",
    "except NameError:\n",
    "    print(\"Loading data from disk...\")\n",
    "    with open('output/Phase_2/processed_data.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        X_processed = data['X_processed']\n",
    "        y = data['y']\n",
    "\n",
    "print(\"\\nMaking predictions on training data (sanity check):\")\n",
    "\n",
    "predictions, probabilities, confidence = predict_new_samples(final_pipeline, X_processed)\n",
    "\n",
    "print(\"\\nFirst 10 samples:\")\n",
    "print(f\"{'Sample':<8} {'True':^6} {'Pred':^6} {'Probability':^12} {'Confidence':^10} {'Status':^8}\")\n",
    "\n",
    "for i in range(min(10, len(predictions))):\n",
    "    status = \"MATCH\" if predictions[i] == y[i] else \"MISS\"\n",
    "    label_true = \"MDD\" if y[i] == 1 else \"Control\"\n",
    "    label_pred = \"MDD\" if predictions[i] == 1 else \"Control\"\n",
    "    \n",
    "    print(f\"{i+1:<8} {label_true:^6} {label_pred:^6} {probabilities[i]:^12.3f} {confidence[i]:^10.3f} {status:^8}\")\n",
    "\n",
    "accuracy = np.mean(predictions == y)\n",
    "print(f\"\\nTraining set accuracy: {accuracy:.3f}\")\n",
    "print(\"(Should be high since model was trained on this data)\")\n",
    "\n",
    "print(\"\\nPrediction Confidence Distribution:\")\n",
    "\n",
    "low_conf = np.sum(confidence < 0.2)\n",
    "med_conf = np.sum((confidence >= 0.2) & (confidence < 0.4))\n",
    "high_conf = np.sum(confidence >= 0.4)\n",
    "\n",
    "print(f\"  High confidence (>0.4):      {high_conf:3d} samples ({high_conf/len(confidence):.1%})\")\n",
    "print(f\"  Medium confidence (0.2-0.4): {med_conf:3d} samples ({med_conf/len(confidence):.1%})\")\n",
    "print(f\"  Low confidence (<0.2):       {low_conf:3d} samples ({low_conf/len(confidence):.1%})\")\n",
    "\n",
    "print(\"\\nSaving standalone prediction function...\")\n",
    "\n",
    "prediction_code = '''\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "def predict_mdd(X_new, model_path='output/Phase_2/final_model.pkl'):\n",
    "    \"\"\"\n",
    "    Predict MDD status for new samples.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_new : array-like, shape (n_samples, n_genes)\n",
    "        Preprocessed gene expression data (log2 + z-score normalized)\n",
    "    model_path : str\n",
    "        Path to saved model file\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    predictions : array (0=Control, 1=MDD)\n",
    "    probabilities : array (probability of MDD)\n",
    "    confidence : array (prediction confidence)\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(model_path, 'rb') as f:\n",
    "        pipeline = pickle.load(f)\n",
    "    \n",
    "    X_selected = pipeline['selector'].transform(X_new)\n",
    "    X_scaled = pipeline['scaler'].transform(X_selected)\n",
    "    predictions = pipeline['classifier'].predict(X_scaled)\n",
    "    probabilities = pipeline['classifier'].predict_proba(X_scaled)[:, 1]\n",
    "    confidence = np.abs(probabilities - 0.5)\n",
    "    \n",
    "    return predictions, probabilities, confidence\n",
    "'''\n",
    "\n",
    "with open('output/Phase_2/predict_mdd.py', 'w') as f:\n",
    "    f.write(prediction_code)\n",
    "\n",
    "print(\"Standalone prediction function saved to 'output/Phase_2/predict_mdd.py'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1c419552-e962-4577-8c07-525683a75a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CREATING VISUALIZATIONS\n",
      "================================================================================\n",
      "\n",
      "Loading all results...\n",
      "  Approach 1 loaded\n",
      "  Approach 2 loaded\n",
      "  Approach 3 loaded\n",
      "  Approach 4 loaded\n",
      "  Approach 5 loaded\n",
      "  Optimal pipeline loaded\n",
      "  Final model loaded\n",
      "\n",
      "All available data loaded\n",
      "\n",
      "Creating Figure 1: Overall Performance Comparison...\n",
      "  Saved: output/Phase_2/figure1_performance_comparison.png\n",
      "Creating Figure 2: Performance Variability Across Folds...\n",
      "  Saved: output/Phase_2/figure2_performance_variability.png\n",
      "Creating Figure 3: ROC-AUC with Confidence Intervals...\n",
      "  Saved: output/Phase_2/figure3_roc_auc_confidence.png\n",
      "Creating Figure 4: Confusion Matrix Heatmap...\n",
      "  Saved: output/Phase_2/figure4_confusion_matrix.png\n",
      "Creating Figure 5: Feature Selection Analysis...\n",
      "  Saved: output/Phase_2/figure5_feature_analysis.png\n",
      "Creating Figure 6: Prediction Probability Distribution...\n",
      "  Saved: output/Phase_2/figure6_prediction_analysis.png\n",
      "Creating Figure 7: Comprehensive Radar Chart...\n",
      "  Saved: output/Phase_2/figure7_radar_chart.png\n",
      "\n",
      "================================================================================\n",
      "VISUALIZATION SUMMARY\n",
      "================================================================================\n",
      "\n",
      "All visualizations created successfully!\n",
      "\n",
      "Generated Figures:\n",
      "   1. figure1_performance_comparison.png     - Bar charts with error bars\n",
      "   2. figure2_performance_variability.png    - Box plots showing CV fold variation\n",
      "   3. figure3_roc_auc_confidence.png         - ROC-AUC with confidence intervals\n",
      "   4. figure4_confusion_matrix.png           - Confusion matrix heatmaps\n",
      "   5. figure5_feature_analysis.png           - Feature selection analysis (4 panels)\n",
      "   6. figure6_prediction_analysis.png        - Prediction distribution & ROC (4 panels)\n",
      "   7. figure7_radar_chart.png                - Multi-metric radar comparison\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.labelsize'] = 11\n",
    "plt.rcParams['axes.titlesize'] = 12\n",
    "plt.rcParams['xtick.labelsize'] = 9\n",
    "plt.rcParams['ytick.labelsize'] = 9\n",
    "plt.rcParams['legend.fontsize'] = 9\n",
    "plt.rcParams['figure.titlesize'] = 14\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CREATING VISUALIZATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nLoading all results...\")\n",
    "\n",
    "approach_results = {}\n",
    "for i in range(1, 6):\n",
    "    try:\n",
    "        with open(f'output/Phase_2/results_approach{i}.pkl', 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            if isinstance(data, dict) and 'results' in data:\n",
    "                approach_results[f'Approach {i}'] = data['results']\n",
    "            else:\n",
    "                approach_results[f'Approach {i}'] = data\n",
    "        print(f\"  Approach {i} loaded\")\n",
    "    except:\n",
    "        print(f\"  Approach {i} not found\")\n",
    "\n",
    "try:\n",
    "    with open('output/Phase_2/results_optimal.pkl', 'rb') as f:\n",
    "        optimal_results = pickle.load(f)\n",
    "    print(f\"  Optimal pipeline loaded\")\n",
    "except:\n",
    "    print(f\"  Optimal pipeline not found\")\n",
    "    optimal_results = None\n",
    "\n",
    "try:\n",
    "    with open('output/Phase_2/final_model.pkl', 'rb') as f:\n",
    "        final_model = pickle.load(f)\n",
    "    print(f\"  Final model loaded\")\n",
    "except:\n",
    "    print(f\"  Final model not found\")\n",
    "    final_model = None\n",
    "\n",
    "print(\"\\nAll available data loaded\\n\")\n",
    "\n",
    "print(\"Creating Figure 1: Overall Performance Comparison...\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "fig.suptitle('Performance Comparison Across All Approaches', fontsize=16, fontweight='bold')\n",
    "\n",
    "metrics = ['accuracy', 'roc_auc', 'mcc']\n",
    "titles = ['Accuracy', 'ROC-AUC', 'MCC (Matthews Correlation)']\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "\n",
    "for idx, (metric, title, color) in enumerate(zip(metrics, titles, colors)):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    approach_names = []\n",
    "    means = []\n",
    "    stds = []\n",
    "    \n",
    "    for name, results in approach_results.items():\n",
    "        approach_names.append(name.replace('Approach ', 'A'))\n",
    "        means.append(np.mean(results[metric]))\n",
    "        stds.append(np.std(results[metric]))\n",
    "    \n",
    "    if optimal_results:\n",
    "        approach_names.append('Optimal')\n",
    "        if metric == 'accuracy':\n",
    "            means.append(optimal_results['metrics']['accuracy'])\n",
    "        elif metric == 'roc_auc':\n",
    "            means.append(optimal_results['metrics']['auc'])\n",
    "        elif metric == 'mcc':\n",
    "            means.append(optimal_results['metrics']['mcc'])\n",
    "        stds.append(0)\n",
    "    \n",
    "    x_pos = np.arange(len(approach_names))\n",
    "    bars = ax.bar(x_pos, means, yerr=stds, capsize=5, \n",
    "                   color=color, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "    \n",
    "    best_idx = np.argmax(means)\n",
    "    bars[best_idx].set_alpha(1.0)\n",
    "    bars[best_idx].set_edgecolor('gold')\n",
    "    bars[best_idx].set_linewidth(3)\n",
    "    \n",
    "    ax.set_xlabel('Approach', fontweight='bold')\n",
    "    ax.set_ylabel(title, fontweight='bold')\n",
    "    ax.set_title(title, fontweight='bold')\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(approach_names, rotation=45, ha='right')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    ax.set_ylim([0, 1.0])\n",
    "    \n",
    "    if metric in ['accuracy', 'roc_auc']:\n",
    "        ax.axhline(y=0.5, color='red', linestyle='--', alpha=0.5, linewidth=1, label='Random')\n",
    "    if metric == 'roc_auc':\n",
    "        ax.axhline(y=0.7, color='orange', linestyle='--', alpha=0.5, linewidth=1, label='Good')\n",
    "        ax.axhline(y=0.8, color='green', linestyle='--', alpha=0.5, linewidth=1, label='Excellent')\n",
    "    \n",
    "    for i, (m, s) in enumerate(zip(means, stds)):\n",
    "        ax.text(i, m + s + 0.02, f'{m:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/Phase_2/figure1_performance_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(\"  Saved: output/Phase_2/figure1_performance_comparison.png\")\n",
    "plt.close()\n",
    "\n",
    "print(\"Creating Figure 2: Performance Variability Across Folds...\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "fig.suptitle('Performance Variability Across Cross-Validation Folds', fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, (metric, title) in enumerate(zip(metrics, titles)):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    data_for_box = []\n",
    "    labels_for_box = []\n",
    "    \n",
    "    for name, results in approach_results.items():\n",
    "        data_for_box.append(results[metric])\n",
    "        labels_for_box.append(name.replace('Approach ', 'A'))\n",
    "    \n",
    "    bp = ax.boxplot(data_for_box, labels=labels_for_box, patch_artist=True,\n",
    "                     showmeans=True, meanline=True)\n",
    "    \n",
    "    colors_box = plt.cm.Set3(np.linspace(0, 1, len(data_for_box)))\n",
    "    for patch, color in zip(bp['boxes'], colors_box):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.7)\n",
    "    \n",
    "    for element in ['whiskers', 'fliers', 'means', 'medians', 'caps']:\n",
    "        plt.setp(bp[element], color='black', linewidth=1.5)\n",
    "    plt.setp(bp['means'], color='red', linewidth=2)\n",
    "    plt.setp(bp['medians'], color='blue', linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel('Approach', fontweight='bold')\n",
    "    ax.set_ylabel(title, fontweight='bold')\n",
    "    ax.set_title(f'{title} Distribution', fontweight='bold')\n",
    "    ax.set_xticklabels(labels_for_box, rotation=45, ha='right')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    ax.set_ylim([0, 1.0])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/Phase_2/figure2_performance_variability.png', dpi=300, bbox_inches='tight')\n",
    "print(\"  Saved: output/Phase_2/figure2_performance_variability.png\")\n",
    "plt.close()\n",
    "\n",
    "print(\"Creating Figure 3: ROC-AUC with Confidence Intervals...\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "fig.suptitle('ROC-AUC Scores with 95% Confidence Intervals', fontsize=16, fontweight='bold')\n",
    "\n",
    "approach_names = []\n",
    "auc_means = []\n",
    "auc_stds = []\n",
    "auc_mins = []\n",
    "auc_maxs = []\n",
    "\n",
    "for name, results in approach_results.items():\n",
    "    approach_names.append(name.replace('Approach ', 'A'))\n",
    "    auc_data = results['roc_auc']\n",
    "    auc_means.append(np.mean(auc_data))\n",
    "    auc_stds.append(np.std(auc_data))\n",
    "    auc_mins.append(np.min(auc_data))\n",
    "    auc_maxs.append(np.max(auc_data))\n",
    "\n",
    "if optimal_results:\n",
    "    approach_names.append('Optimal')\n",
    "    auc_means.append(optimal_results['metrics']['auc'])\n",
    "    auc_stds.append(0)\n",
    "    auc_mins.append(optimal_results['metrics']['auc'])\n",
    "    auc_maxs.append(optimal_results['metrics']['auc'])\n",
    "\n",
    "x_pos = np.arange(len(approach_names))\n",
    "\n",
    "ax.plot(x_pos, auc_means, 'o-', linewidth=2, markersize=10, color='#e74c3c', label='Mean AUC')\n",
    "\n",
    "ci_lower = np.array(auc_means) - 1.96 * np.array(auc_stds)\n",
    "ci_upper = np.array(auc_means) + 1.96 * np.array(auc_stds)\n",
    "ax.fill_between(x_pos, ci_lower, ci_upper, alpha=0.3, color='#e74c3c', label='95% CI')\n",
    "\n",
    "ax.fill_between(x_pos, auc_mins, auc_maxs, alpha=0.1, color='gray', label='Min-Max Range')\n",
    "\n",
    "ax.axhline(y=0.5, color='red', linestyle='--', alpha=0.5, linewidth=2, label='Random (0.5)')\n",
    "ax.axhline(y=0.7, color='orange', linestyle='--', alpha=0.5, linewidth=2, label='Good (0.7)')\n",
    "ax.axhline(y=0.8, color='green', linestyle='--', alpha=0.5, linewidth=2, label='Excellent (0.8)')\n",
    "\n",
    "ax.set_xlabel('Approach', fontweight='bold', fontsize=12)\n",
    "ax.set_ylabel('ROC-AUC Score', fontweight='bold', fontsize=12)\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(approach_names, rotation=45, ha='right')\n",
    "ax.set_ylim([0, 1.0])\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(loc='lower right', framealpha=0.9)\n",
    "\n",
    "best_idx = np.argmax(auc_means)\n",
    "ax.annotate(f'Best: {auc_means[best_idx]:.3f}', \n",
    "            xy=(best_idx, auc_means[best_idx]), \n",
    "            xytext=(best_idx, auc_means[best_idx] + 0.1),\n",
    "            arrowprops=dict(arrowstyle='->', color='gold', lw=2),\n",
    "            fontsize=11, fontweight='bold', color='gold',\n",
    "            bbox=dict(boxstyle='round,pad=0.5', facecolor='black', alpha=0.7))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/Phase_2/figure3_roc_auc_confidence.png', dpi=300, bbox_inches='tight')\n",
    "print(\"  Saved: output/Phase_2/figure3_roc_auc_confidence.png\")\n",
    "plt.close()\n",
    "\n",
    "if optimal_results:\n",
    "    print(\"Creating Figure 4: Confusion Matrix Heatmap...\")\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    fig.suptitle('Optimal Pipeline: Confusion Matrix Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    cm = optimal_results['metrics']['confusion_matrix']\n",
    "    \n",
    "    ax = axes[0]\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True, \n",
    "                square=True, ax=ax, linewidths=2, linecolor='black',\n",
    "                annot_kws={'size': 16, 'weight': 'bold'})\n",
    "    ax.set_xlabel('Predicted Label', fontweight='bold', fontsize=12)\n",
    "    ax.set_ylabel('True Label', fontweight='bold', fontsize=12)\n",
    "    ax.set_title('Confusion Matrix (Counts)', fontweight='bold')\n",
    "    ax.set_xticklabels(['Control', 'MDD'], fontsize=11)\n",
    "    ax.set_yticklabels(['Control', 'MDD'], fontsize=11, rotation=0)\n",
    "    \n",
    "    ax = axes[1]\n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    sns.heatmap(cm_norm, annot=True, fmt='.2%', cmap='RdYlGn', cbar=True,\n",
    "                square=True, ax=ax, linewidths=2, linecolor='black',\n",
    "                annot_kws={'size': 16, 'weight': 'bold'}, vmin=0, vmax=1)\n",
    "    ax.set_xlabel('Predicted Label', fontweight='bold', fontsize=12)\n",
    "    ax.set_ylabel('True Label', fontweight='bold', fontsize=12)\n",
    "    ax.set_title('Confusion Matrix (Normalized)', fontweight='bold')\n",
    "    ax.set_xticklabels(['Control', 'MDD'], fontsize=11)\n",
    "    ax.set_yticklabels(['Control', 'MDD'], fontsize=11, rotation=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('output/Phase_2/figure4_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"  Saved: output/Phase_2/figure4_confusion_matrix.png\")\n",
    "    plt.close()\n",
    "\n",
    "print(\"Creating Figure 5: Feature Selection Analysis...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Feature Selection Analysis Across Approaches', fontsize=16, fontweight='bold')\n",
    "\n",
    "ax = axes[0, 0]\n",
    "approach_names_feat = []\n",
    "n_features = []\n",
    "\n",
    "for name, results in approach_results.items():\n",
    "    if 'n_features' in results:\n",
    "        approach_names_feat.append(name.replace('Approach ', 'A'))\n",
    "        n_features.append(np.mean(results['n_features']))\n",
    "\n",
    "if len(n_features) > 0:\n",
    "    bars = ax.barh(approach_names_feat, n_features, color='skyblue', edgecolor='black', linewidth=1.5)\n",
    "    ax.set_xlabel('Average Number of Features Selected', fontweight='bold')\n",
    "    ax.set_ylabel('Approach', fontweight='bold')\n",
    "    ax.set_title('A) Feature Selection Size', fontweight='bold', loc='left')\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    for i, v in enumerate(n_features):\n",
    "        ax.text(v + max(n_features)*0.02, i, f'{v:.0f}', va='center', fontweight='bold')\n",
    "else:\n",
    "    ax.text(0.5, 0.5, 'Feature count data not available', \n",
    "            ha='center', va='center', transform=ax.transAxes, fontsize=12)\n",
    "    ax.set_title('A) Feature Selection Size', fontweight='bold', loc='left')\n",
    "\n",
    "ax = axes[0, 1]\n",
    "if optimal_results and 'gene_selection_frequency' in optimal_results['metrics']:\n",
    "    gene_freq = optimal_results['metrics']['gene_selection_frequency']\n",
    "    \n",
    "    ax.hist(gene_freq[gene_freq > 0], bins=50, color='coral', edgecolor='black', alpha=0.7)\n",
    "    ax.set_xlabel('Gene Selection Frequency', fontweight='bold')\n",
    "    ax.set_ylabel('Number of Genes', fontweight='bold')\n",
    "    ax.set_title('B) Gene Selection Stability (Optimal)', fontweight='bold', loc='left')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    consistent = np.sum(gene_freq > 0.5)\n",
    "    ax.axvline(x=0.5, color='red', linestyle='--', linewidth=2, label=f'50% threshold ({consistent} genes)')\n",
    "    ax.legend()\n",
    "else:\n",
    "    ax.text(0.5, 0.5, 'Gene stability data not available', \n",
    "            ha='center', va='center', transform=ax.transAxes, fontsize=12)\n",
    "    ax.set_title('B) Gene Selection Stability', fontweight='bold', loc='left')\n",
    "\n",
    "ax = axes[1, 0]\n",
    "if final_model and 'gene_importance' in final_model:\n",
    "    gene_imp = final_model['gene_importance'].head(15)\n",
    "    \n",
    "    y_pos = np.arange(len(gene_imp))\n",
    "    ax.barh(y_pos, gene_imp['stability_score'], color='lightgreen', edgecolor='black', linewidth=1.5)\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(gene_imp['gene'], fontsize=8)\n",
    "    ax.set_xlabel('Stability Score', fontweight='bold')\n",
    "    ax.set_ylabel('Gene', fontweight='bold')\n",
    "    ax.set_title('C) Top 15 Most Stable Genes', fontweight='bold', loc='left')\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    ax.invert_yaxis()\n",
    "else:\n",
    "    ax.text(0.5, 0.5, 'Gene importance data not available', \n",
    "            ha='center', va='center', transform=ax.transAxes, fontsize=12)\n",
    "    ax.set_title('C) Top Genes', fontweight='bold', loc='left')\n",
    "\n",
    "ax = axes[1, 1]\n",
    "perf_vs_feat = []\n",
    "\n",
    "for name, results in approach_results.items():\n",
    "    if 'n_features' in results:\n",
    "        mean_auc = np.mean(results['roc_auc'])\n",
    "        mean_features = np.mean(results['n_features'])\n",
    "        perf_vs_feat.append({\n",
    "            'approach': name.replace('Approach ', 'A'),\n",
    "            'auc': mean_auc,\n",
    "            'features': mean_features\n",
    "        })\n",
    "\n",
    "if len(perf_vs_feat) > 0:\n",
    "    df_pf = pd.DataFrame(perf_vs_feat)\n",
    "    \n",
    "    scatter = ax.scatter(df_pf['features'], df_pf['auc'], s=200, c=df_pf['auc'], \n",
    "                         cmap='RdYlGn', edgecolors='black', linewidth=2, alpha=0.8)\n",
    "    \n",
    "    for _, row in df_pf.iterrows():\n",
    "        ax.annotate(row['approach'], (row['features'], row['auc']), \n",
    "                    fontsize=9, ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    ax.set_xlabel('Number of Features', fontweight='bold')\n",
    "    ax.set_ylabel('ROC-AUC', fontweight='bold')\n",
    "    ax.set_title('D) Performance vs Feature Count', fontweight='bold', loc='left')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    cbar = plt.colorbar(scatter, ax=ax)\n",
    "    cbar.set_label('ROC-AUC', fontweight='bold')\n",
    "else:\n",
    "    ax.text(0.5, 0.5, 'Performance vs Features data not available', \n",
    "            ha='center', va='center', transform=ax.transAxes, fontsize=12)\n",
    "    ax.set_title('D) Performance vs Features', fontweight='bold', loc='left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/Phase_2/figure5_feature_analysis.png', dpi=300, bbox_inches='tight')\n",
    "print(\"  Saved: output/Phase_2/figure5_feature_analysis.png\")\n",
    "plt.close()\n",
    "\n",
    "if optimal_results:\n",
    "    print(\"Creating Figure 6: Prediction Probability Distribution...\")\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle('Optimal Pipeline: Prediction Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    predictions = np.array(optimal_results['predictions'])\n",
    "    probabilities = np.array(optimal_results['probabilities'])\n",
    "    true_labels = np.array(optimal_results['true_labels'])\n",
    "    \n",
    "    ax = axes[0, 0]\n",
    "    control_probs = probabilities[true_labels == 0]\n",
    "    mdd_probs = probabilities[true_labels == 1]\n",
    "    \n",
    "    ax.hist(control_probs, bins=20, alpha=0.6, label='True Control', color='blue', edgecolor='black')\n",
    "    ax.hist(mdd_probs, bins=20, alpha=0.6, label='True MDD', color='red', edgecolor='black')\n",
    "    ax.axvline(x=0.5, color='black', linestyle='--', linewidth=2, label='Decision Threshold')\n",
    "    ax.set_xlabel('Predicted Probability (MDD)', fontweight='bold')\n",
    "    ax.set_ylabel('Frequency', fontweight='bold')\n",
    "    ax.set_title('A) Probability Distribution by True Class', fontweight='bold', loc='left')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    ax = axes[0, 1]\n",
    "    confidence = np.abs(probabilities - 0.5)\n",
    "    correct = predictions == true_labels\n",
    "    \n",
    "    ax.scatter(np.arange(len(probabilities))[correct], confidence[correct], \n",
    "               c='green', label='Correct', s=60, alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "    ax.scatter(np.arange(len(probabilities))[~correct], confidence[~correct], \n",
    "               c='red', label='Incorrect', s=60, alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "    \n",
    "    ax.axhline(y=0.2, color='orange', linestyle='--', alpha=0.5, linewidth=2, label='Low Confidence')\n",
    "    ax.axhline(y=0.4, color='green', linestyle='--', alpha=0.5, linewidth=2, label='High Confidence')\n",
    "    \n",
    "    ax.set_xlabel('Sample Index', fontweight='bold')\n",
    "    ax.set_ylabel('Prediction Confidence', fontweight='bold')\n",
    "    ax.set_title('B) Prediction Confidence per Sample', fontweight='bold', loc='left')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax = axes[1, 0]\n",
    "    sorted_idx = np.argsort(probabilities)\n",
    "    sorted_probs = probabilities[sorted_idx]\n",
    "    sorted_true = true_labels[sorted_idx]\n",
    "    \n",
    "    cumsum_true_pos = np.cumsum(sorted_true)\n",
    "    cumsum_false_pos = np.cumsum(1 - sorted_true)\n",
    "    \n",
    "    tpr = cumsum_true_pos / cumsum_true_pos[-1] if cumsum_true_pos[-1] > 0 else cumsum_true_pos\n",
    "    fpr = cumsum_false_pos / cumsum_false_pos[-1] if cumsum_false_pos[-1] > 0 else cumsum_false_pos\n",
    "    \n",
    "    ax.plot(fpr, tpr, linewidth=3, color='darkblue', label=f\"AUC = {optimal_results['metrics']['auc']:.3f}\")\n",
    "    ax.plot([0, 1], [0, 1], 'r--', linewidth=2, label='Random (AUC = 0.5)')\n",
    "    ax.fill_between(fpr, tpr, alpha=0.3, color='blue')\n",
    "    \n",
    "    ax.set_xlabel('False Positive Rate', fontweight='bold')\n",
    "    ax.set_ylabel('True Positive Rate', fontweight='bold')\n",
    "    ax.set_title('C) ROC Curve', fontweight='bold', loc='left')\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1])\n",
    "    \n",
    "    ax = axes[1, 1]\n",
    "    \n",
    "    n_bins = 10\n",
    "    bins = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "    \n",
    "    binned_true = []\n",
    "    binned_count = []\n",
    "    \n",
    "    for i in range(n_bins):\n",
    "        mask = (probabilities >= bins[i]) & (probabilities < bins[i+1])\n",
    "        if np.sum(mask) > 0:\n",
    "            binned_true.append(np.mean(true_labels[mask]))\n",
    "            binned_count.append(np.sum(mask))\n",
    "        else:\n",
    "            binned_true.append(np.nan)\n",
    "            binned_count.append(0)\n",
    "    \n",
    "    valid_mask = ~np.isnan(binned_true)\n",
    "    ax.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Perfect Calibration')\n",
    "    ax.scatter(bin_centers[valid_mask], np.array(binned_true)[valid_mask], \n",
    "               s=np.array(binned_count)[valid_mask]*10, c='purple', \n",
    "               alpha=0.6, edgecolors='black', linewidth=2, label='Observed')\n",
    "    \n",
    "    ax.set_xlabel('Mean Predicted Probability', fontweight='bold')\n",
    "    ax.set_ylabel('Fraction of Positives', fontweight='bold')\n",
    "    ax.set_title('D) Calibration Plot', fontweight='bold', loc='left')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('output/Phase_2/figure6_prediction_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"  Saved: output/Phase_2/figure6_prediction_analysis.png\")\n",
    "    plt.close()\n",
    "\n",
    "print(\"Creating Figure 7: Comprehensive Radar Chart...\")\n",
    "\n",
    "from matplotlib.patches import Circle, RegularPolygon\n",
    "from matplotlib.path import Path\n",
    "from matplotlib.projections.polar import PolarAxes\n",
    "from matplotlib.projections import register_projection\n",
    "from matplotlib.spines import Spine\n",
    "from matplotlib.transforms import Affine2D\n",
    "\n",
    "def radar_factory(num_vars, frame='circle'):\n",
    "    theta = np.linspace(0, 2 * np.pi, num_vars, endpoint=False)\n",
    "    \n",
    "    class RadarAxes(PolarAxes):\n",
    "        name = 'radar'\n",
    "        \n",
    "        def __init__(self, *args, **kwargs):\n",
    "            super().__init__(*args, **kwargs)\n",
    "            self.set_theta_zero_location('N')\n",
    "        \n",
    "        def fill(self, *args, closed=True, **kwargs):\n",
    "            return super().fill(closed=closed, *args, **kwargs)\n",
    "        \n",
    "        def plot(self, *args, **kwargs):\n",
    "            lines = super().plot(*args, **kwargs)\n",
    "            for line in lines:\n",
    "                self._close_line(line)\n",
    "        \n",
    "        def _close_line(self, line):\n",
    "            x, y = line.get_data()\n",
    "            if x[0] != x[-1]:\n",
    "                x = np.append(x, x[0])\n",
    "                y = np.append(y, y[0])\n",
    "                line.set_data(x, y)\n",
    "        \n",
    "        def set_varlabels(self, labels):\n",
    "            self.set_thetagrids(np.degrees(theta), labels)\n",
    "        \n",
    "        def _gen_axes_patch(self):\n",
    "            return Circle((0.5, 0.5), 0.5)\n",
    "        \n",
    "        def _gen_axes_spines(self):\n",
    "            spine = Spine(axes=self, spine_type='circle', path=Path.unit_circle())\n",
    "            spine.set_transform(Affine2D().scale(.5).translate(.5, .5) + self.transAxes)\n",
    "            return {'polar': spine}\n",
    "    \n",
    "    register_projection(RadarAxes)\n",
    "    return theta\n",
    "\n",
    "categories = ['Accuracy', 'ROC-AUC', 'MCC', 'Precision', 'Sensitivity', 'Specificity']\n",
    "N = len(categories)\n",
    "\n",
    "theta = radar_factory(N, frame='polygon')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='radar'))\n",
    "fig.suptitle('Multi-Metric Performance Radar Chart', fontsize=16, fontweight='bold', y=0.98)\n",
    "\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8']\n",
    "color_idx = 0\n",
    "\n",
    "if optimal_results:\n",
    "    cm = optimal_results['metrics']['confusion_matrix']\n",
    "    tn, fp, fn, tp = cm[0,0], cm[0,1], cm[1,0], cm[1,1]\n",
    "    \n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    \n",
    "    values = [\n",
    "        optimal_results['metrics']['accuracy'],\n",
    "        optimal_results['metrics']['auc'],\n",
    "        (optimal_results['metrics']['mcc'] + 1) / 2,\n",
    "        precision,\n",
    "        sensitivity,\n",
    "        specificity\n",
    "    ]\n",
    "    values += values[:1]\n",
    "    \n",
    "    ax.plot(theta, values[:-1], 'o-', linewidth=2, color=colors[color_idx], \n",
    "            label='Optimal', markersize=8)\n",
    "    ax.fill(theta, values[:-1], alpha=0.15, color=colors[color_idx])\n",
    "    color_idx += 1\n",
    "\n",
    "top_approaches = sorted(approach_results.items(), \n",
    "                        key=lambda x: np.mean(x[1]['roc_auc']), \n",
    "                        reverse=True)[:2]\n",
    "\n",
    "for name, results in top_approaches:\n",
    "    if color_idx >= len(colors):\n",
    "        break\n",
    "    \n",
    "    values = [\n",
    "        np.mean(results['accuracy']),\n",
    "        np.mean(results['roc_auc']),\n",
    "        (np.mean(results['mcc']) + 1) / 2,\n",
    "        np.mean(results['accuracy']),\n",
    "        np.mean(results['accuracy']),\n",
    "        np.mean(results['accuracy'])\n",
    "    ]\n",
    "    values += values[:1]\n",
    "    \n",
    "    ax.plot(theta, values[:-1], 'o-', linewidth=2, color=colors[color_idx], \n",
    "            label=name, markersize=6)\n",
    "    ax.fill(theta, values[:-1], alpha=0.1, color=colors[color_idx])\n",
    "    color_idx += 1\n",
    "\n",
    "ax.set_varlabels(categories)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.grid(True)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/Phase_2/figure7_radar_chart.png', dpi=300, bbox_inches='tight')\n",
    "print(\"  Saved: output/Phase_2/figure7_radar_chart.png\")\n",
    "plt.close()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VISUALIZATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nAll visualizations created successfully!\\n\")\n",
    "print(\"Generated Figures:\")\n",
    "print(\"   1. figure1_performance_comparison.png     - Bar charts with error bars\")\n",
    "print(\"   2. figure2_performance_variability.png    - Box plots showing CV fold variation\")\n",
    "print(\"   3. figure3_roc_auc_confidence.png         - ROC-AUC with confidence intervals\")\n",
    "print(\"   4. figure4_confusion_matrix.png           - Confusion matrix heatmaps\")\n",
    "print(\"   5. figure5_feature_analysis.png           - Feature selection analysis (4 panels)\")\n",
    "print(\"   6. figure6_prediction_analysis.png        - Prediction distribution & ROC (4 panels)\")\n",
    "print(\"   7. figure7_radar_chart.png                - Multi-metric radar comparison\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acfc782-5bce-467f-b523-672c8b297d6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e324db3-4a0f-4397-99bb-c7ec15d98e09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9c4a97-3d54-47aa-9ed0-1f3cae361e94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
